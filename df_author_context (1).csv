Author,context_left,context_right
Devlin et al.,". LinkBERT is
especially effective for multi-hop reasoning and
few-shot QA (+5% absolute improvement on
HotpotQA and TriviaQA), and our biomedical
LinkBERT sets new states of the art on various
BioNLP tasks (+7% on BioASQ and USMLE).
We release our pretrained models, LinkBERT
andBioLinkBERT , as well as code and data.1
1 Introduction
Pretrained language models (LMs), like BERT and
GPTs (","Devlin et al., 2019; Brown et al., 2020), have
shown remarkable performance on many natural
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (Devlin
et al., 2019), LMs learn to encode various knowl-
edge from"
 Brown et al.,"ally effective for multi-hop reasoning and
few-shot QA (+5% absolute improvement on
HotpotQA and TriviaQA), and our biomedical
LinkBERT sets new states of the art on various
BioNLP tasks (+7% on BioASQ and USMLE).
We release our pretrained models, LinkBERT
andBioLinkBERT , as well as code and data.1
1 Introduction
Pretrained language models (LMs), like BERT and
GPTs (Devlin et al., 2019;"," Brown et al., 2020), have
shown remarkable performance on many natural
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (Devlin
et al., 2019), LMs learn to encode various knowl-
edge from text corpora and pr"
"Bommasani
et al.","ur biomedical
LinkBERT sets new states of the art on various
BioNLP tasks (+7% on BioASQ and USMLE).
We release our pretrained models, LinkBERT
andBioLinkBERT , as well as code and data.1
1 Introduction
Pretrained language models (LMs), like BERT and
GPTs (Devlin et al., 2019; Brown et al., 2020), have
shown remarkable performance on many natural
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (","Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (Devlin
et al., 2019), LMs learn to encode various knowl-
edge from text corpora and produce informative
representations for downstream tasks (Petroni et al.,
2019; Bosselut et al., 2019; Raffel et al., 2020).
Equal senior authorship.
1Available at https://github.com/michiyasunaga/
LinkBERT .
Language Model[CLS] The Tidal Basin [SEP] [MASK] [MASK] trees... ... [SEP] Segment AS"
"Devlin
et al.","ntroduction
Pretrained language models (LMs), like BERT and
GPTs (Devlin et al., 2019; Brown et al., 2020), have
shown remarkable performance on many natural
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (","Devlin
et al., 2019), LMs learn to encode various knowl-
edge from text corpora and produce informative
representations for downstream tasks (Petroni et al.,
2019; Bosselut et al., 2019; Raffel et al., 2020).
Equal senior authorship.
1Available at https://github.com/michiyasunaga/
LinkBERT .
Language Model[CLS] The Tidal Basin [SEP] [MASK] [MASK] trees... ... [SEP] Segment ASegment BJap"
Petroni et al.,"
shown remarkable performance on many natural
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (Devlin
et al., 2019), LMs learn to encode various knowl-
edge from text corpora and produce informative
representations for downstream tasks (","Petroni et al.,
2019; Bosselut et al., 2019; Raffel et al., 2020).
Equal senior authorship.
1Available at https://github.com/michiyasunaga/
LinkBERT .
Language Model[CLS] The Tidal Basin [SEP] [MASK] [MASK] trees... ... [SEP] Segment ASegment BJapanese cherryMasked language modeling (MLM)Document relation prediction (DRP)●Linked ●Random ●Contiguous 
Corpus with Document Links (e.g. hyperlink, reference)Doc 2Doc 5Doc"
Bosselut et al.,"0), have
shown remarkable performance on many natural
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (Devlin
et al., 2019), LMs learn to encode various knowl-
edge from text corpora and produce informative
representations for downstream tasks (Petroni et al.,
2019; ","Bosselut et al., 2019; Raffel et al., 2020).
Equal senior authorship.
1Available at https://github.com/michiyasunaga/
LinkBERT .
Language Model[CLS] The Tidal Basin [SEP] [MASK] [MASK] trees... ... [SEP] Segment ASegment BJapanese cherryMasked language modeling (MLM)Document relation prediction (DRP)●Linked ●Random ●Contiguous 
Corpus with Document Links (e.g. hyperlink, reference)Doc 2Doc 5Doc 4Doc 6Doc 1Doc 3
CreateLM inputsTrain LMsSegment AS"
 Raffel et al.,"l
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (Devlin
et al., 2019), LMs learn to encode various knowl-
edge from text corpora and produce informative
representations for downstream tasks (Petroni et al.,
2019; Bosselut et al., 2019;"," Raffel et al., 2020).
Equal senior authorship.
1Available at https://github.com/michiyasunaga/
LinkBERT .
Language Model[CLS] The Tidal Basin [SEP] [MASK] [MASK] trees... ... [SEP] Segment ASegment BJapanese cherryMasked language modeling (MLM)Document relation prediction (DRP)●Linked ●Random ●Contiguous 
Corpus with Document Links (e.g. hyperlink, reference)Doc 2Doc 5Doc 4Doc 6Doc 1Doc 3
CreateLM inputsTrain LMsSe"
Liu et al.,"us applications, including
answering a question “What trees can you see at Tidal Basin?”.
We aim to leverage document links to incorporate more
knowledge into language model pretraining.
However, existing LM pretraining methods typ-
ically consider text from a single document in each
input context (","Liu et al., 2019; Joshi et al., 2020)
and do not model links between documents. This
can pose limitations because documents often have
rich dependencies (e.g. hyperlinks, references), and
knowledge can span across documents. As an exam-
ple, in Figure 1, the Wikipedia article “Tidal Basin,
Washingto"
 Joshi et al.,". “Tidal Basin has Japanese cherry
trees”), which can be useful for various applications, including
answering a question “What trees can you see at Tidal Basin?”.
We aim to leverage document links to incorporate more
knowledge into language model pretraining.
However, existing LM pretraining methods typ-
ically consider text from a single document in each
input context (Liu et al., 2019;"," Joshi et al., 2020)
and do not model links between documents. This
can pose limitations because documents often have
rich dependencies (e.g. hyperlinks, references), and
knowledge can span across documents. As an exam-
ple, in Figure 1, the Wikipedia article “Tidal Basin,
Washington D.C.” (left) describes that the basin
hosts “National Cherry Blossom Festival”, and the
hyperlinked artic"
Margolis et al.,"ontext ( linked ), besides the existing options of placing a single document ( contiguous ) or a pair
of random documents ( random ) as in BERT. We then train the LM with two self-supervised objectives: masked language modeling
(MLM), which predicts masked tokens in the input, and document relation prediction (DRP), which classiﬁes the relation of
the two text segments in the input ( contiguous ,random , orlinked ) (§4.2).
even make discoveries (","Margolis et al., 1999).
In this work, we propose LinkBERT , an effective
language model pretraining method that incor-
porates document link knowledge. Given a text
corpus, we obtain links between documents such as
hyperlinks, and create LM inputs by placing linked
documents in the same context, besides the existing
option of placing a single document or random doc-
uments as in BERT. Speciﬁcally, as in Figure 2, after
sampling an anchor text seg"
"Yang
et al.","ween
documents, beyond the ability learned in the vanilla
next sentence prediction objective in BERT.
Viewing the pretraining corpus as a graph
of documents, LinkBERT is also motivated as
self-supervised learning on the graph, where DRP
and MLM correspond to link prediction and node
feature prediction in graph machine learning (","Yang
et al., 2015; Hu et al., 2020). Our modeling approach
thus provides a natural fusion of language-based
and graph-based self-supervised learning.
We train LinkBERT in two domains: the general
domain, using Wikipedia articles with hyperlinks
(§4), and the biomedical domain, using PubMed ar-
ticles with citation links (§6). We"
 Hu et al.,"e vanilla
next sentence prediction objective in BERT.
Viewing the pretraining corpus as a graph
of documents, LinkBERT is also motivated as
self-supervised learning on the graph, where DRP
and MLM correspond to link prediction and node
feature prediction in graph machine learning (Yang
et al., 2015;"," Hu et al., 2020). Our modeling approach
thus provides a natural fusion of language-based
and graph-based self-supervised learning.
We train LinkBERT in two domains: the general
domain, using Wikipedia articles with hyperlinks
(§4), and the biomedical domain, using PubMed ar-
ticles with citation li"
Gu et al.," in both domains.LinkBERT consistently improves on baseline LMs
across domains and tasks. For the general domain,
LinkBERT outperforms BERT on MRQA bench-
mark (+4% absolute in F1-score) as well as GLUE
benchmark. For the biomedical domain, LinkBERT
exceeds PubmedBERT (","Gu et al., 2020) and sets
new states of the art on BLURB biomedical NLP
benchmark (+3% absolute in BLURB score) and
MedQA-USMLE reasoning task (+7% absolute in
accuracy). Overall, LinkBERT attains notably large
gains for multi-hop reasoning, multi-document
understanding"
Lewis et al.,"soning task (+7% absolute in
accuracy). Overall, LinkBERT attains notably large
gains for multi-hop reasoning, multi-document
understanding, and few-shot question answering,
suggesting that LinkBERT internalizes signiﬁcantly
more knowledge than existing LMs by pretraining
with document link information.
2 Related work
Retrieval-augmented LMs. Several works
(","Lewis et al., 2020b; Karpukhin et al., 2020; Oguz
et al., 2020; Xie et al., 2022) introduce a retrieval
module for LMs, where given an anchor text
(e.g. question), retrieved text is added to the same
LM context to improve model inference (e.g. an-
swer prediction). These works show the promise of
placing related documents in the same LM context
at inference "
 Karpukhin et al.,"u et al., 2020) and sets
new states of the art on BLURB biomedical NLP
benchmark (+3% absolute in BLURB score) and
MedQA-USMLE reasoning task (+7% absolute in
accuracy). Overall, LinkBERT attains notably large
gains for multi-hop reasoning, multi-document
understanding, and few-shot question answering,
suggesting that LinkBERT internalizes signiﬁcantly
more knowledge than existing LMs by pretraining
with document link information.
2 Related work
Retrieval-augmented LMs. Several works
(Lewis et al., 2020b;"," Karpukhin et al., 2020; Oguz
et al., 2020; Xie et al., 2022) introduce a retrieval
module for LMs, where given an anchor text
(e.g. question), retrieved text is added to the same
LM context to improve model inference (e.g. an-
swer prediction). These works show the promise of
placing related documents in the same LM context
at inference time, but they do not study the effect of
doing so in pretraining. Guu et al. (2020) pretrain
an LM with a retriever that learns to retrieve text for
answering masked tok"
"Oguz
et al.","arge
gains for multi-hop reasoning, multi-document
understanding, and few-shot question answering,
suggesting that LinkBERT internalizes signiﬁcantly
more knowledge than existing LMs by pretraining
with document link information.
2 Related work
Retrieval-augmented LMs. Several works
(Lewis et al., 2020b; Karpukhin et al., 2020; ","Oguz
et al., 2020; Xie et al., 2022) introduce a retrieval
module for LMs, where given an anchor text
(e.g. question), retrieved text is added to the same
LM context to improve model inference (e.g. an-
swer prediction). These works show the promise of
placing related documents in the same LM context
at inference time, but they "
 Xie et al.,"ti-hop reasoning, multi-document
understanding, and few-shot question answering,
suggesting that LinkBERT internalizes signiﬁcantly
more knowledge than existing LMs by pretraining
with document link information.
2 Related work
Retrieval-augmented LMs. Several works
(Lewis et al., 2020b; Karpukhin et al., 2020; Oguz
et al., 2020;"," Xie et al., 2022) introduce a retrieval
module for LMs, where given an anchor text
(e.g. question), retrieved text is added to the same
LM context to improve model inference (e.g. an-
swer prediction). These works show the promise of
placing related documents in the same LM context
at inference time, but they do not study the e"
Guu et al. (2020,"Related work
Retrieval-augmented LMs. Several works
(Lewis et al., 2020b; Karpukhin et al., 2020; Oguz
et al., 2020; Xie et al., 2022) introduce a retrieval
module for LMs, where given an anchor text
(e.g. question), retrieved text is added to the same
LM context to improve model inference (e.g. an-
swer prediction). These works show the promise of
placing related documents in the same LM context
at inference time, but they do not study the effect of
doing so in pretraining. ","Guu et al. (2020) pretrain
an LM with a retriever that learns to retrieve text for
answering masked tokens in the anchor text. In con-
trast, our focus is not on retrieval, but on pretraining
a general-purpose LM that internalizes knowledge
that spans across documents, which is orthogonal
to the above works (e.g., our pretrained LM could
be used to initialize the LM component of these
works). Additionally, we focus on incorporating
document links such as hyperlinks, which can"
Asai et al.," knowledge
that spans across documents, which is orthogonal
to the above works (e.g., our pretrained LM could
be used to initialize the LM component of these
works). Additionally, we focus on incorporating
document links such as hyperlinks, which can offer
salient knowledge that common lexical retrieval
methods may not provide (","Asai et al., 2020).
Pretrain LMs with related documents. Several
concurrent works use multiple related documentsto pretrain LMs. Caciularu et al. (2021) place doc-
uments (news articles) about the same topic into the
same LM context, and Levine et al. (2021) place sen-
tences of high lexical similarity into the same con-
text. O"
Caciularu et al. (2021," LM with a retriever that learns to retrieve text for
answering masked tokens in the anchor text. In con-
trast, our focus is not on retrieval, but on pretraining
a general-purpose LM that internalizes knowledge
that spans across documents, which is orthogonal
to the above works (e.g., our pretrained LM could
be used to initialize the LM component of these
works). Additionally, we focus on incorporating
document links such as hyperlinks, which can offer
salient knowledge that common lexical retrieval
methods may not provide (Asai et al., 2020).
Pretrain LMs with related documents. Several
concurrent works use multiple related documentsto pretrain LMs. ","Caciularu et al. (2021) place doc-
uments (news articles) about the same topic into the
same LM context, and Levine et al. (2021) place sen-
tences of high lexical similarity into the same con-
text. Our work provides a general method to incor-
porate document links into LM pretraining, where
lexical or topical similarity can be one instance of
document links, besides hyperlinks. We focus on hy-
perlinks in this work, because we ﬁnd they can bring
in salient knowledge that may not be obvious via
lexical similarity, and yield a more performant LM
(§5.5). Additionally, we propose the DRP objective,
which improves modeling multiple documents and
relations"
Levine et al. (2021,"es knowledge
that spans across documents, which is orthogonal
to the above works (e.g., our pretrained LM could
be used to initialize the LM component of these
works). Additionally, we focus on incorporating
document links such as hyperlinks, which can offer
salient knowledge that common lexical retrieval
methods may not provide (Asai et al., 2020).
Pretrain LMs with related documents. Several
concurrent works use multiple related documentsto pretrain LMs. Caciularu et al. (2021) place doc-
uments (news articles) about the same topic into the
same LM context, and ","Levine et al. (2021) place sen-
tences of high lexical similarity into the same con-
text. Our work provides a general method to incor-
porate document links into LM pretraining, where
lexical or topical similarity can be one instance of
document links, besides hyperlinks. We focus on hy-
perlinks in this work, because we ﬁnd they can bring
in salient knowledge that may not be obvious via
lexical similarity, and yield a more performant LM
(§5.5). Additionally, we propose the DRP objective,
which improves modeling multiple documents and
relations between them in LM"
Chang et al.,"we ﬁnd they can bring
in salient knowledge that may not be obvious via
lexical similarity, and yield a more performant LM
(§5.5). Additionally, we propose the DRP objective,
which improves modeling multiple documents and
relations between them in LMs (§5.5).
Hyperlinks and citation links for NLP. Hyper-
links are often used to learn better retrieval models.
","Chang et al. (2020); Asai et al. (2020); Seonwoo
et al. (2021) use Wikipedia hyperlinks to train
retrievers for open-domain question answering.
Ma et al. (2021) study various hyperlink-aware
pretraining tasks for retrieval. While these works
use hyperlinks to learn retrievers, we focus on using
hyperlinks to create better context for learning
general-purpose"
 Asai et al. (2020,"o LM pretraining, where
lexical or topical similarity can be one instance of
document links, besides hyperlinks. We focus on hy-
perlinks in this work, because we ﬁnd they can bring
in salient knowledge that may not be obvious via
lexical similarity, and yield a more performant LM
(§5.5). Additionally, we propose the DRP objective,
which improves modeling multiple documents and
relations between them in LMs (§5.5).
Hyperlinks and citation links for NLP. Hyper-
links are often used to learn better retrieval models.
Chang et al. (2020);"," Asai et al. (2020); Seonwoo
et al. (2021) use Wikipedia hyperlinks to train
retrievers for open-domain question answering.
Ma et al. (2021) study various hyperlink-aware
pretraining tasks for retrieval. While these works
use hyperlinks to learn retrievers, we focus on using
hyperlinks to create better context for learning
general-purpose LMs. Separately, Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian an"
"Seonwoo
et al. (2021","hod to incor-
porate document links into LM pretraining, where
lexical or topical similarity can be one instance of
document links, besides hyperlinks. We focus on hy-
perlinks in this work, because we ﬁnd they can bring
in salient knowledge that may not be obvious via
lexical similarity, and yield a more performant LM
(§5.5). Additionally, we propose the DRP objective,
which improves modeling multiple documents and
relations between them in LMs (§5.5).
Hyperlinks and citation links for NLP. Hyper-
links are often used to learn better retrieval models.
Chang et al. (2020); Asai et al. (2020); ","Seonwoo
et al. (2021) use Wikipedia hyperlinks to train
retrievers for open-domain question answering.
Ma et al. (2021) study various hyperlink-aware
pretraining tasks for retrieval. While these works
use hyperlinks to learn retrievers, we focus on using
hyperlinks to create better context for learning
general-purpose LMs. Separately, Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 20"
Ma et al. (2021,"t be obvious via
lexical similarity, and yield a more performant LM
(§5.5). Additionally, we propose the DRP objective,
which improves modeling multiple documents and
relations between them in LMs (§5.5).
Hyperlinks and citation links for NLP. Hyper-
links are often used to learn better retrieval models.
Chang et al. (2020); Asai et al. (2020); Seonwoo
et al. (2021) use Wikipedia hyperlinks to train
retrievers for open-domain question answering.
","Ma et al. (2021) study various hyperlink-aware
pretraining tasks for retrieval. While these works
use hyperlinks to learn retrievers, we focus on using
hyperlinks to create better context for learning
general-purpose LMs. Separately, Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; Yasunaga et al.,
20"
"Calixto et al.
(2021","ally, we propose the DRP objective,
which improves modeling multiple documents and
relations between them in LMs (§5.5).
Hyperlinks and citation links for NLP. Hyper-
links are often used to learn better retrieval models.
Chang et al. (2020); Asai et al. (2020); Seonwoo
et al. (2021) use Wikipedia hyperlinks to train
retrievers for open-domain question answering.
Ma et al. (2021) study various hyperlink-aware
pretraining tasks for retrieval. While these works
use hyperlinks to learn retrievers, we focus on using
hyperlinks to create better context for learning
general-purpose LMs. Separately, ","Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM train"
Qazvinian and Radev,"ter retrieval models.
Chang et al. (2020); Asai et al. (2020); Seonwoo
et al. (2021) use Wikipedia hyperlinks to train
retrievers for open-domain question answering.
Ma et al. (2021) study various hyperlink-aware
pretraining tasks for retrieval. While these works
use hyperlinks to learn retrievers, we focus on using
hyperlinks to create better context for learning
general-purpose LMs. Separately, Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (","Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasunaga et al.
(2021); Zhang et al. (2022) combine LMs and graph
neural networks (GNNs) to joint"
Yasunaga et al.,"uestion answering.
Ma et al. (2021) study various hyperlink-aware
pretraining tasks for retrieval. While these works
use hyperlinks to learn retrievers, we focus on using
hyperlinks to create better context for learning
general-purpose LMs. Separately, Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; ","Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasu"
Bhagavatula et al.,"021) use Wikipedia hyperlinks to train
retrievers for open-domain question answering.
Ma et al. (2021) study various hyperlink-aware
pretraining tasks for retrieval. While these works
use hyperlinks to learn retrievers, we focus on using
hyperlinks to create better context for learning
general-purpose LMs. Separately, Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; ","Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasunaga et al.
(2021); Zhang et al. (2022) combine LMs and graph
neural networks (GNNs) to jointly train on text and"
 Khadka et al.,"tasks for retrieval. While these works
use hyperlinks to learn retrievers, we focus on using
hyperlinks to create better context for learning
general-purpose LMs. Separately, Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018;"," Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasunaga et al.
(2021)"
Cohan et al.,"s on using
hyperlinks to create better context for learning
general-purpose LMs. Separately, Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
","Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun"
Zhang et al.,"ers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. ","Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasunaga et al.
(2021); Zhang et al. (2022) combine LMs and graph
neural networks (GNNs) to jointly train on text and
KGs. Different from KGs, we use document graphs
to learn knowledge that spans across documents.
3 Preliminaries
A language m"
 He et al. (2020,"
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019);"," He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasunaga et al.
(2021); Zhang et al. (2022) combine LMs and graph
neural networks (GNNs) to jointly train on text and
KGs. Different from KGs, we use document graphs
to learn knowledge that spans across documents.
3 Preliminaries
A language model (LM) can be pretrained from a
corpus of documents, X=fX(i)g. An LM is a com-
position of two functions, fhead(fenc(X)), where
the encod"
Wang et al. (2021,"n multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
","Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasunaga et al.
(2021); Zhang et al. (2022) combine LMs and graph
neural networks (GNNs) to jointly train on text and
KGs. Different from KGs, we use document graphs
to learn knowledge that spans across documents.
3 Preliminaries
A language model (LM) can be pretrained from a
corpus of documents, X=fX(i)g. An LM is a com-
position of two functions, fhead(fenc(X)), where
the encoderfenctakes in a sequence of tokens X=
(x1;x2;:::"
Sun et al.," to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. ","Sun et al. (2020); Yasunaga et al.
(2021); Zhang et al. (2022) combine LMs and graph
neural networks (GNNs) to jointly train on text and
KGs. Different from KGs, we use document graphs
to learn knowledge that spans across documents.
3 Preliminaries
A language model (LM) can be pretrained from a
corp"
" Yasunaga et al.
(2021"," LMs. Separately, Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020);"," Yasunaga et al.
(2021); Zhang et al. (2022) combine LMs and graph
neural networks (GNNs) to jointly train on text and
KGs. Different from KGs, we use document graphs
to learn knowledge that spans across documents.
3 Preliminaries
A language model (LM) can be pretrained from a
corpus of documents, X=fX(i)g. An LM is a com-
position of two functions, fhead(fenc(X)), where
the encoderfenctakes in a sequence of tokens X=
(x1;x2;:::;x n)and produces a contextualized vector
representation for each token, (h1;h2;:::;hn). The
headfheaduses these representations to perform self-
supervised tasks in the pretraining step and to per-
form downstream tasks in the "
Zhang et al. (2022,"ion and recommendation of academic
papers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasunaga et al.
(2021); ","Zhang et al. (2022) combine LMs and graph
neural networks (GNNs) to jointly train on text and
KGs. Different from KGs, we use document graphs
to learn knowledge that spans across documents.
3 Preliminaries
A language model (LM) can be pretrained from a
corpus of documents, X=fX(i)g. An LM is a com-
position of two functions, fhead(fenc(X)), where
the encoderfenctakes in a sequence of tokens X=
(x1;x2;:::;x n)and produces a contextualized vector
representation for each token, (h1;h2;:::;hn). The
headfheaduses these representations to p"
Devlin et al.,". LinkBERT is
especially effective for multi-hop reasoning and
few-shot QA (+5% absolute improvement on
HotpotQA and TriviaQA), and our biomedical
LinkBERT sets new states of the art on various
BioNLP tasks (+7% on BioASQ and USMLE).
We release our pretrained models, LinkBERT
andBioLinkBERT , as well as code and data.1
1 Introduction
Pretrained language models (LMs), like BERT and
GPTs (","Devlin et al., 2019; Brown et al., 2020), have
shown remarkable performance on many natural
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (Devlin
et al., 2019), LMs learn to encode various knowl-
edge from"
"Aghajanyan
et al.","§6). Hy-
perlinks have a number of advantages. They provide
background knowledge about concepts that the doc-
ument writers deemed useful—the links are likely
to have high precision of relevance, and can also
bring in relevant documents that may not be obvious
via lexical similarity alone (e.g., in Figure 1, while
the hyperlinked article mentions “Japanese” and
“Yoshino” cherry trees, these words do not appear in
the anchor article). Hyperlinks are also ubiquitous
on the web and easily gathered at scale (","Aghajanyan
et al., 2021). To construct the document graph, we
simply make a directed edge (X(i);X(j))if there is
a hyperlink from document X(i)to document X(j).
For comparison, we also experiment with a docu-
ment graph built by lexical similarity between docu-
ments. For each document X(i), we use the common
TF-IDF cosine similarity metric (Chen et al., 2017;
Yasunaga et al., 2017) to obtain top- kdocuments
X(j)’s and make edges (X(i);X(j)). We usek=5.
4.2 Pretraining tasks
Creating input instances. Seve"
Chen et al.,"al., 2021). To construct the document graph, we
simply make a directed edge (X(i);X(j))if there is
a hyperlink from document X(i)to document X(j).
For comparison, we also experiment with a docu-
ment graph built by lexical similarity between docu-
ments. For each document X(i), we use the common
TF-IDF cosine similarity metric (","Chen et al., 2017;
Yasunaga et al., 2017) to obtain top- kdocuments
X(j)’s and make edges (X(i);X(j)). We usek=5.
4.2 Pretraining tasks
Creating input instances. Several works (Gao
et al., 2021; Levine et al., 2021) ﬁnd that LMs can
learn stronger dependencies between words that
were shown together in the same context during
tra"
Yasunaga et al.,"uestion answering.
Ma et al. (2021) study various hyperlink-aware
pretraining tasks for retrieval. While these works
use hyperlinks to learn retrievers, we focus on using
hyperlinks to create better context for learning
general-purpose LMs. Separately, Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; ","Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasu"
"Gao
et al.","uilt by lexical similarity between docu-
ments. For each document X(i), we use the common
TF-IDF cosine similarity metric (Chen et al., 2017;
Yasunaga et al., 2017) to obtain top- kdocuments
X(j)’s and make edges (X(i);X(j)). We usek=5.
4.2 Pretraining tasks
Creating input instances. Several works (","Gao
et al., 2021; Levine et al., 2021) ﬁnd that LMs can
learn stronger dependencies between words that
were shown together in the same context during
training, than words that were not. To effectively
learn knowledge that spans across documents, we
create LM inputs by placing linked documents in
the"
 Levine et al.,"t of these
works). Additionally, we focus on incorporating
document links such as hyperlinks, which can offer
salient knowledge that common lexical retrieval
methods may not provide (Asai et al., 2020).
Pretrain LMs with related documents. Several
concurrent works use multiple related documentsto pretrain LMs. Caciularu et al. (2021) place doc-
uments (news articles) about the same topic into the
same LM context, and"," Levine et al. (2021) place sen-
tences of high lexical similarity into the same con-
text. Our work provides a general method to incor-
porate document links into LM pretraining, where
lexical or topical similarity can be one instance of
document links, besides hyperlinks. We focus on hy-
perlinks in this work, because we ﬁnd they can bring
in salient knowledge that may not be obvious via
lexical similarity, and yie"
Hu et al.,"ion objective in BERT.
Viewing the pretraining corpus as a graph
of documents, LinkBERT is also motivated as
self-supervised learning on the graph, where DRP
and MLM correspond to link prediction and node
feature prediction in graph machine learning (Yang
et al., 2015; ","Hu et al., 2020). Our modeling approach
thus provides a natural fusion of language-based
and graph-based self-supervised learning.
We train LinkBERT in two domains: the general
domain, using Wikipedia articles with hyperlinks
(§4), and the biomedical domain, using PubMe"
Bordes et al.,"d
link prediction, are commonly used to learn the
content and structure of a graph. In node feature
prediction (Hu et al., 2020), some features of a node
are masked, and the task is to predict them using
neighbor nodes. This corresponds to our MLM
task, where masked tokens in Segment A can be
predicted using Segment B (a linked document
on the graph), and vice versa. In link prediction
(","Bordes et al., 2013; Wang et al., 2021a), the task is
to predict the existence or type of an edge between
two nodes. This corresponds to our DRP task,
where we predict if the given pair of text segments
are linked (edge), contiguous (self-loop edge), or
random (no edge). Our approach can be viewed as
a natural fusion of language-based (e.g. BERT) and
graph-based self-supervised learning."
 Wang et al.,"content and structure of a graph. In node feature
prediction (Hu et al., 2020), some features of a node
are masked, and the task is to predict them using
neighbor nodes. This corresponds to our MLM
task, where masked tokens in Segment A can be
predicted using Segment B (a linked document
on the graph), and vice versa. In link prediction
(Bordes et al., 2013;"," Wang et al., 2021a), the task is
to predict the existence or type of an edge between
two nodes. This corresponds to our DRP task,
where we predict if the given pair of text segments
are linked (edge), contiguous (self-loop edge), or
random (no edge). Our approach can be viewed as
a natural fusion of language-based (e.g. BERT) and
graph-based self-supervised"
Zhang et al.,"ers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. ","Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasunaga et al.
(2021); Zhang et al. (2022) combine LMs and graph
neural networks (GNNs) to jointly train on text and
KGs. Different from KGs, we use document graphs
to learn knowledge that spans across documents.
3 Preliminaries
A language m"
"Asai
et al.","that may not be ob-
vious to the current LM. Hyperlinks are potentially
more advantageous than lexical similarity links in
this regard: LMs are shown to be good at recogniz-
ing lexical similarity (Zhang et al., 2020), and hyper-
links can bring in useful background knowledge thatmay not be obvious via lexical similarity alone (","Asai
et al., 2020). Indeed, we empirically ﬁnd that using
hyperlinks yields a more performant LM (§5.5).
Diversity. In the document graph, some docu-
ments may have a very high in-degree (e.g., many
incoming hyperlinks, like the “United States” page
of Wikipedia), and others a low in-degree. If we uni-
formly sample from the lin"
Henzinger et al.,"
incoming hyperlinks, like the “United States” page
of Wikipedia), and others a low in-degree. If we uni-
formly sample from the linked documents for each
anchor segment, we may include documents of high
in-degree too often in the overall training data, los-
ing diversity. To adjust so that all documents appear
with a similar frequency in training, we sample a
linked document with probability inversely propor-
tional to its in-degree, as done in graph data mining
literature (","Henzinger et al., 2000). We ﬁnd that this
technique yields a better LM performance (§5.5).
5 Experiments
We experiment with our proposed approach in the
general domain ﬁrst, where we pretrain LinkBERT
on Wikipedia articles with hyperlinks (§5.1) and
evaluate on a suite of downstream tasks (§5.2). We
compare with BERT (Devlin et al., 2019) as our base-
line. We experiment in the biomedical domain in §6.
5.1 Pretraining setup
Data. We use the same pretraining corpus used
by BER"
Devlin et al.,". LinkBERT is
especially effective for multi-hop reasoning and
few-shot QA (+5% absolute improvement on
HotpotQA and TriviaQA), and our biomedical
LinkBERT sets new states of the art on various
BioNLP tasks (+7% on BioASQ and USMLE).
We release our pretrained models, LinkBERT
andBioLinkBERT , as well as code and data.1
1 Introduction
Pretrained language models (LMs), like BERT and
GPTs (","Devlin et al., 2019; Brown et al., 2020), have
shown remarkable performance on many natural
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (Devlin
et al., 2019), LMs learn to encode various knowl-
edge from"
Zhu et al.,"edia articles with hyperlinks (§5.1) and
evaluate on a suite of downstream tasks (§5.2). We
compare with BERT (Devlin et al., 2019) as our base-
line. We experiment in the biomedical domain in §6.
5.1 Pretraining setup
Data. We use the same pretraining corpus used
by BERT: Wikipedia and BookCorpus (","Zhu et al.,
2015). For Wikipedia, we use the WikiExtractor3to
extract hyperlinks between Wiki articles. We then
create training instances by sampling contiguous ,
random , orlinked segments as described in §4, with
the three options appearing uniformly (33%, 33%,
33%). For BookCorpus, we create trai"
Devlin et al.,". LinkBERT is
especially effective for multi-hop reasoning and
few-shot QA (+5% absolute improvement on
HotpotQA and TriviaQA), and our biomedical
LinkBERT sets new states of the art on various
BioNLP tasks (+7% on BioASQ and USMLE).
We release our pretrained models, LinkBERT
andBioLinkBERT , as well as code and data.1
1 Introduction
Pretrained language models (LMs), like BERT and
GPTs (","Devlin et al., 2019; Brown et al., 2020), have
shown remarkable performance on many natural
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (Devlin
et al., 2019), LMs learn to encode various knowl-
edge from"
 Turc et al.," and BookCorpus to train
LinkBERT. In summary, our pretraining data is
the same as BERT, except that we have hyperlinks
between Wikipedia articles.
Implementation. We pretrain LinkBERT of
three sizes, -tiny, -base and -large, following the
conﬁgurations of BERT tiny (4.4M parameters),
BERT base(110M params), and BERT large (340M
params) (Devlin et al., 2019;"," Turc et al., 2019). We
use -tiny mainly for ablation studies.
For -tiny, we pretrain from scratch with ran-
dom weight initialization. We use the AdamW
(Loshchilov and Hutter, 2019) optimizer with
(1;2) = (0:9;0:98), warm up the learning rate
for the ﬁrst 5,000 steps and then linearly decay it.
3https://github.com/attardi/wikiextractorWe train for 10,000 "
Loshchilov and Hutter,"y sampling contiguous orrandom segments (50%,
50%) as in BERT. We then combine the training
instances from Wikipedia and BookCorpus to train
LinkBERT. In summary, our pretraining data is
the same as BERT, except that we have hyperlinks
between Wikipedia articles.
Implementation. We pretrain LinkBERT of
three sizes, -tiny, -base and -large, following the
conﬁgurations of BERT tiny (4.4M parameters),
BERT base(110M params), and BERT large (340M
params) (Devlin et al., 2019; Turc et al., 2019). We
use -tiny mainly for ablation studies.
For -tiny, we pretrain from scratch with ran-
dom weight initialization. We use the AdamW
(","Loshchilov and Hutter, 2019) optimizer with
(1;2) = (0:9;0:98), warm up the learning rate
for the ﬁrst 5,000 steps and then linearly decay it.
3https://github.com/attardi/wikiextractorWe train for 10,000 steps with a peak learning rate
5e-3, weight decay 0.01, and batch size of 2,048
sequences with 512 tokens. Training took 1 day on
two GeForce RTX 2080 Ti GPUs with fp16.
For -base, we initialize LinkBERT with the
BERT base checkpoint released by Devlin et al.
(2019) and continue pretraining. We use a peak
learning rate 3e-4 and train for 40,000 steps. Other
training hyperparameters are the same as -tiny.
Training took 4"
"Devlin et al.
(2019","nly for ablation studies.
For -tiny, we pretrain from scratch with ran-
dom weight initialization. We use the AdamW
(Loshchilov and Hutter, 2019) optimizer with
(1;2) = (0:9;0:98), warm up the learning rate
for the ﬁrst 5,000 steps and then linearly decay it.
3https://github.com/attardi/wikiextractorWe train for 10,000 steps with a peak learning rate
5e-3, weight decay 0.01, and batch size of 2,048
sequences with 512 tokens. Training took 1 day on
two GeForce RTX 2080 Ti GPUs with fp16.
For -base, we initialize LinkBERT with the
BERT base checkpoint released by ","Devlin et al.
(2019) and continue pretraining. We use a peak
learning rate 3e-4 and train for 40,000 steps. Other
training hyperparameters are the same as -tiny.
Training took 4 days on four A100 GPUs with fp16.
For -large, we follow the same procedure as
-base, except that we use a peak learning rate of 2e-4.
Training took 7 days on eight A100 GPUs with fp16.
Baselines. We compare LinkBERT with BERT.
Speciﬁcally, for the -tiny scale, we compare with
BERT tiny, which we pretrain from scratch with the
same hyperparameters as LinkBERT tiny. The only
difference is th"
"Devlin et al.
(2019","nly for ablation studies.
For -tiny, we pretrain from scratch with ran-
dom weight initialization. We use the AdamW
(Loshchilov and Hutter, 2019) optimizer with
(1;2) = (0:9;0:98), warm up the learning rate
for the ﬁrst 5,000 steps and then linearly decay it.
3https://github.com/attardi/wikiextractorWe train for 10,000 steps with a peak learning rate
5e-3, weight decay 0.01, and batch size of 2,048
sequences with 512 tokens. Training took 1 day on
two GeForce RTX 2080 Ti GPUs with fp16.
For -base, we initialize LinkBERT with the
BERT base checkpoint released by ","Devlin et al.
(2019) and continue pretraining. We use a peak
learning rate 3e-4 and train for 40,000 steps. Other
training hyperparameters are the same as -tiny.
Training took 4 days on four A100 GPUs with fp16.
For -large, we follow the same procedure as
-base, except that we use a peak learning rate of 2e-4.
Training took 7 days on eight A100 GPUs with fp16.
Baselines. We compare LinkBERT with BERT.
Speciﬁcally, for the -tiny scale, we compare with
BERT tiny, which we pretrain from scratch with the
same hyperparameters as LinkBERT tiny. The only
difference is th"
Fisch et al.,"
For -large, we follow the same procedure as -base.
5.2 Evaluation tasks
We ﬁne-tune and evaluate LinkBERT on a suite of
downstream tasks.
Extractive question answering (QA). Given a
document (or set of documents) and a question as
input, the task is to identify an answer span from
the document. We evaluate on six popular datasets
from the MRQA shared task (","Fisch et al., 2019):
HotpotQA (Yang et al., 2018), TriviaQA (Joshi
et al., 2017), NaturalQ (Kwiatkowski et al., 2019),
SearchQA (Dunn et al., 2017), NewsQA (Trischler
et al., 2017), and SQuAD (Rajpurkar et al., 2016).
As the MRQA shared task does not have a public
test set, we split the dev set in half to make new
dev and test sets. We follow the ﬁne-tuning "
Yang et al.,"ation tasks
We ﬁne-tune and evaluate LinkBERT on a suite of
downstream tasks.
Extractive question answering (QA). Given a
document (or set of documents) and a question as
input, the task is to identify an answer span from
the document. We evaluate on six popular datasets
from the MRQA shared task (Fisch et al., 2019):
HotpotQA (","Yang et al., 2018), TriviaQA (Joshi
et al., 2017), NaturalQ (Kwiatkowski et al., 2019),
SearchQA (Dunn et al., 2017), NewsQA (Trischler
et al., 2017), and SQuAD (Rajpurkar et al., 2016).
As the MRQA shared task does not have a public
test set, we split the dev set in half to make new
dev and test sets. We follow the ﬁne-tuning m"
"Joshi
et al.","ation tasks
We ﬁne-tune and evaluate LinkBERT on a suite of
downstream tasks.
Extractive question answering (QA). Given a
document (or set of documents) and a question as
input, the task is to identify an answer span from
the document. We evaluate on six popular datasets
from the MRQA shared task (Fisch et al., 2019):
HotpotQA (Yang et al., 2018), TriviaQA (","Joshi
et al., 2017), NaturalQ (Kwiatkowski et al., 2019),
SearchQA (Dunn et al., 2017), NewsQA (Trischler
et al., 2017), and SQuAD (Rajpurkar et al., 2016).
As the MRQA shared task does not have a public
test set, we split the dev set in half to make new
dev and test sets. We follow the ﬁne-tuning method
BERT (Devlin et al., 2019) uses for extractive QA.
Mor"
Dunn et al.,"answering (QA). Given a
document (or set of documents) and a question as
input, the task is to identify an answer span from
the document. We evaluate on six popular datasets
from the MRQA shared task (Fisch et al., 2019):
HotpotQA (Yang et al., 2018), TriviaQA (Joshi
et al., 2017), NaturalQ (Kwiatkowski et al., 2019),
SearchQA (","Dunn et al., 2017), NewsQA (Trischler
et al., 2017), and SQuAD (Rajpurkar et al., 2016).
As the MRQA shared task does not have a public
test set, we split the dev set in half to make new
dev and test sets. We follow the ﬁne-tuning method
BERT (Devlin et al., 2019) uses for extractive QA.
More details are provided in Appendix B.
"
"Trischler
et al.","dure as -base.
5.2 Evaluation tasks
We ﬁne-tune and evaluate LinkBERT on a suite of
downstream tasks.
Extractive question answering (QA). Given a
document (or set of documents) and a question as
input, the task is to identify an answer span from
the document. We evaluate on six popular datasets
from the MRQA shared task (Fisch et al., 2019):
HotpotQA (Yang et al., 2018), TriviaQA (Joshi
et al., 2017), NaturalQ (Kwiatkowski et al., 2019),
SearchQA (Dunn et al., 2017), NewsQA (","Trischler
et al., 2017), and SQuAD (Rajpurkar et al., 2016).
As the MRQA shared task does not have a public
test set, we split the dev set in half to make new
dev and test sets. We follow the ﬁne-tuning method
BERT (Devlin et al., 2019) uses for extractive QA.
More details are provided in Appendix B.
GLUE. The General Language Understanding
Evaluation (GLUE) benchmark (Wang et al., 2018)
is a popular suite of sentence-level classiﬁcation
tasks. Following BERT, we evaluate on "
Devlin et al.,". LinkBERT is
especially effective for multi-hop reasoning and
few-shot QA (+5% absolute improvement on
HotpotQA and TriviaQA), and our biomedical
LinkBERT sets new states of the art on various
BioNLP tasks (+7% on BioASQ and USMLE).
We release our pretrained models, LinkBERT
andBioLinkBERT , as well as code and data.1
1 Introduction
Pretrained language models (LMs), like BERT and
GPTs (","Devlin et al., 2019; Brown et al., 2020), have
shown remarkable performance on many natural
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (Devlin
et al., 2019), LMs learn to encode various knowl-
edge from"
Wang et al.," al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
","Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasunaga et al.
(2021); Zhang et al. (2022) combine LMs and graph
neural networks (GNNs) to jointly train on text and
KGs. Different from KGs, we use document graphs
to learn knowledge that spans across documents.
3 Preliminaries
A language model (LM)"
Warstadt et al.,"Rajpurkar et al., 2016).
As the MRQA shared task does not have a public
test set, we split the dev set in half to make new
dev and test sets. We follow the ﬁne-tuning method
BERT (Devlin et al., 2019) uses for extractive QA.
More details are provided in Appendix B.
GLUE. The General Language Understanding
Evaluation (GLUE) benchmark (Wang et al., 2018)
is a popular suite of sentence-level classiﬁcation
tasks. Following BERT, we evaluate on CoLA
(","Warstadt et al., 2019), SST-2 (Socher et al., 2013),
MRPC (Dolan and Brockett, 2005), QQP ,STS-B
(Cer et al., 2017), MNLI (Williams et al., 2017),
QNLI (Rajpurkar et al., 2016), and RTE (Dagan
et al., 2005; Haim et al., 2006; GiampiccoloHotpotQA TriviaQA SearchQA NaturalQ NewsQA SQuAD Avg.
BERT tiny 49.8 43.4 50.2 58.9 41.3 56.6 50.0
LinkBERT tiny 54.6 50.0 58.6 60.3 42.8 58.0 54.1
BERT base 76.0 70.3 74.2 76.5 65.7 88.7 75.2
LinkBERT base 78.2 7"
Dolan and Brockett,"hler
et al., 2017), and SQuAD (Rajpurkar et al., 2016).
As the MRQA shared task does not have a public
test set, we split the dev set in half to make new
dev and test sets. We follow the ﬁne-tuning method
BERT (Devlin et al., 2019) uses for extractive QA.
More details are provided in Appendix B.
GLUE. The General Language Understanding
Evaluation (GLUE) benchmark (Wang et al., 2018)
is a popular suite of sentence-level classiﬁcation
tasks. Following BERT, we evaluate on CoLA
(Warstadt et al., 2019), SST-2 (Socher et al., 2013),
MRPC (","Dolan and Brockett, 2005), QQP ,STS-B
(Cer et al., 2017), MNLI (Williams et al., 2017),
QNLI (Rajpurkar et al., 2016), and RTE (Dagan
et al., 2005; Haim et al., 2006; GiampiccoloHotpotQA TriviaQA SearchQA NaturalQ NewsQA SQuAD Avg.
BERT tiny 49.8 43.4 50.2 58.9 41.3 56.6 50.0
LinkBERT tiny 54.6 50.0 58.6 60.3 42.8 58.0 54.1
BERT base 76.0 70.3 74.2 76.5 65.7 88.7 75.2
LinkBERT base 78.2 73.9 76.8 78.3 69.3 90.1 77.8
BERT large 78.1 73.7 78.3 79.0 70.9 91.1 78.5
LinkBERT large 80.8 78.2 80.5 81.0 72.6 92.7 81.0
Table 1: Performance (F1"
Cer et al.,"ed in Appendix B.
GLUE. The General Language Understanding
Evaluation (GLUE) benchmark (Wang et al., 2018)
is a popular suite of sentence-level classiﬁcation
tasks. Following BERT, we evaluate on CoLA
(Warstadt et al., 2019), SST-2 (Socher et al., 2013),
MRPC (Dolan and Brockett, 2005), QQP ,STS-B
(","Cer et al., 2017), MNLI (Williams et al., 2017),
QNLI (Rajpurkar et al., 2016), and RTE (Dagan
et al., 2005; Haim et al., 2006; GiampiccoloHotpotQA TriviaQA SearchQA NaturalQ NewsQA SQuAD Avg.
BERT tiny 49.8 43.4 50.2 58.9 41.3 56.6 50.0
LinkBERT tiny 54.6 50.0 58.6 60.3 42.8 58.0 54.1
BERT base 76."
Rajpurkar et al.,"We ﬁne-tune and evaluate LinkBERT on a suite of
downstream tasks.
Extractive question answering (QA). Given a
document (or set of documents) and a question as
input, the task is to identify an answer span from
the document. We evaluate on six popular datasets
from the MRQA shared task (Fisch et al., 2019):
HotpotQA (Yang et al., 2018), TriviaQA (Joshi
et al., 2017), NaturalQ (Kwiatkowski et al., 2019),
SearchQA (Dunn et al., 2017), NewsQA (Trischler
et al., 2017), and SQuAD (","Rajpurkar et al., 2016).
As the MRQA shared task does not have a public
test set, we split the dev set in half to make new
dev and test sets. We follow the ﬁne-tuning method
BERT (Devlin et al., 2019) uses for extractive QA.
More details are provided in Appendix B.
GLUE. The General Language Understanding
Evaluation (GLUE) benchmark (Wang et al., 2018)
is a popular suite of sentence-level classiﬁcation
tasks. Following BERT, we evaluate on CoLA
(Warstadt et al., 2019), SST-2 "
"Dagan
et al.","eneral Language Understanding
Evaluation (GLUE) benchmark (Wang et al., 2018)
is a popular suite of sentence-level classiﬁcation
tasks. Following BERT, we evaluate on CoLA
(Warstadt et al., 2019), SST-2 (Socher et al., 2013),
MRPC (Dolan and Brockett, 2005), QQP ,STS-B
(Cer et al., 2017), MNLI (Williams et al., 2017),
QNLI (Rajpurkar et al., 2016), and RTE (","Dagan
et al., 2005; Haim et al., 2006; GiampiccoloHotpotQA TriviaQA SearchQA NaturalQ NewsQA SQuAD Avg.
BERT tiny 49.8 43.4 50.2 58.9 41.3 56.6 50.0
LinkBERT tiny 54.6 50.0 58.6 60.3 42.8 58.0 54.1
BERT base 76.0 70.3 74.2 76.5 65.7 88.7 75.2
LinkBERT base 78.2 73.9 76.8 78.3 69.3 90.1 77.8
BERT large 78.1 73.7 78.3 79.0 70.9 91.1 78.5
LinkBERT large 80.8 78"
 Haim et al.,"erstanding
Evaluation (GLUE) benchmark (Wang et al., 2018)
is a popular suite of sentence-level classiﬁcation
tasks. Following BERT, we evaluate on CoLA
(Warstadt et al., 2019), SST-2 (Socher et al., 2013),
MRPC (Dolan and Brockett, 2005), QQP ,STS-B
(Cer et al., 2017), MNLI (Williams et al., 2017),
QNLI (Rajpurkar et al., 2016), and RTE (Dagan
et al., 2005;"," Haim et al., 2006; GiampiccoloHotpotQA TriviaQA SearchQA NaturalQ NewsQA SQuAD Avg.
BERT tiny 49.8 43.4 50.2 58.9 41.3 56.6 50.0
LinkBERT tiny 54.6 50.0 58.6 60.3 42.8 58.0 54.1
BERT base 76.0 70.3 74.2 76.5 65.7 88.7 75.2
LinkBERT base 78.2 73.9 76.8 78.3 69.3 90.1 77.8
BERT large 78.1 73.7 78.3 79.0 70.9 91.1 78.5
LinkBERT large 80.8 78.2 80.5 81.0 72.6 9"
"Chen
et al.","ents
helps for multi-hop reasoning on downstream tasks.
Improved understanding of document relations.
While the MRQA datasets typically use ground-
truth documents as context for answering questions,
in open-domain QA, QA systems need to use
documents obtained by a retriever, which may
include noisy documents besides gold ones (","Chen
et al., 2017; Dunn et al., 2017). In such cases, QA
systems need to understand the document relations
to perform well (Yang et al., 2018). To simulate
this setting, we modify the SQuAD dataset by
prepending or appending 1–2 distracting documents
to the original document given to each question.
Table 3 shows the result. Whil"
 Dunn et al.,"linked documents
helps for multi-hop reasoning on downstream tasks.
Improved understanding of document relations.
While the MRQA datasets typically use ground-
truth documents as context for answering questions,
in open-domain QA, QA systems need to use
documents obtained by a retriever, which may
include noisy documents besides gold ones (Chen
et al., 2017;"," Dunn et al., 2017). In such cases, QA
systems need to understand the document relations
to perform well (Yang et al., 2018). To simulate
this setting, we modify the SQuAD dataset by
prepending or appending 1–2 distracting documents
to the original document given to each question.
Table 3 shows the result. While BERT incurs a large
performance drop (-2.8%), "
Yang et al.,"ation tasks
We ﬁne-tune and evaluate LinkBERT on a suite of
downstream tasks.
Extractive question answering (QA). Given a
document (or set of documents) and a question as
input, the task is to identify an answer span from
the document. We evaluate on six popular datasets
from the MRQA shared task (Fisch et al., 2019):
HotpotQA (","Yang et al., 2018), TriviaQA (Joshi
et al., 2017), NaturalQ (Kwiatkowski et al., 2019),
SearchQA (Dunn et al., 2017), NewsQA (Trischler
et al., 2017), and SQuAD (Rajpurkar et al., 2016).
As the MRQA shared task does not have a public
test set, we split the dev set in half to make new
dev and test sets. We follow the ﬁne-tuning m"
Ansari et al. 2015)Doc B: ... Deep vein thrombosis is tested by compression ultrasonography ... (e.g. Piovella et al. 2002,"(§4.2).
et al., 2007), and report the average score. More
ﬁne-tuning details are provided in Appendix B.
5.3 Results
Table 1 shows the performance (F1 score) on
MRQA datasets. LinkBERT substantially outper-
forms BERT on all datasets. On average, the gain is
+4.1% absolute for the BERT tinyscale, +2.6% for
the BERT basescale, and +2.5% for the BERT large
scale. Table 2 shows the results on GLUE, where
LinkBERT performs moderately better than BERT.
These results suggest that LinkBERT is especially
effective at learning knowledge useful for QA tasks
(e.g. world knowledge), while keeping performance
on sentence-level language understanding.
5.4 Analysis
We further study when LinkBERT is especially
useful in downstream tasks.Improved multi-hop reasoning. In Table 1,
we ﬁnd that LinkBERT obtains notably large
gains on QA datasets that require reasoning with
multiple documents, such as HotpotQA (+5% over
BERT tiny), TriviaQA (+6%) and SearchQA (+8%),
as opposed to SQuAD (+1.4%) which just has
a single document per question. To further gain
qualitative insights, we studied in what QA exam-
ples LinkBERT succeeds but BERT fails. Figure
3 shows a representative example from HotpotQA.
Answering the question needs 2-hop reasoning:
identify “Roden Brothers were taken over by Birks
Group” from the ﬁrst document, and then “Birks
Group is headquartered in Montreal” from the sec-
ond document. While BERT tends to simply predict
an entity near the question entity (“Toronto” in the
ﬁrst document, which is just 1-hop), LinkBERT
correctly predicts the answer in the second docu-
ment (“Montreal”). Our intuition is that because
LinkBERT is pretrained with pairs of linked docu-
ments rather than purely single documents, it better
learns how to ﬂow information (e.g., do attention)
across tokens when multiple related documents
are given in the context. In summary, these results
suggest that pretraining with linked documents
helps for multi-hop reasoning on downstream tasks.
Improved understanding of document relations.
While the MRQA datasets typically use ground-
truth documents as context for answering questions,
in open-domain QA, QA systems need to use
documents obtained by a retriever, which may
include noisy documents besides gold ones (Chen
et al., 2017; Dunn et al., 2017). In such cases, QA
systems need to understand the document relations
to perform well (Yang et al., 2018). To simulate
this setting, we modify the SQuAD dataset by
prepending or appending 1–2 distracting documents
to the original document given to each question.
Table 3 shows the result. While BERT incurs a large
performance drop (-2.8%), LinkBERT is robust to
distracting documents (-0.5%). This result suggests
that pretraining with document links improves
the ability to understand document relations andThree days after undergoing a laparoscopic Whipple's procedure, a 43-year-old woman has swelling of her right leg. ... She was diagnosed with pancreatic cancer 1 month ago. ... Her temperature is 38°C (100.4°F), pulse is 90/min, and blood pressure is 118/78 mm Hg. Examination shows mild swelling of the right thigh to the ankle; there is no erythema or pitting edema. ... Which of the following is the most appropriate next step in management?(A)  CT pulmonary angiography     (B)  Compression ultrasonography(C)  D-dimer level                                 (D)  2 sets of blood culturesLinkBERT predicts: B (✓)    PubmedBERT predicts: D (✗)Leg swelling, pancreatic cancer(symptom) Deep vein thrombosis(possible cause)Compression ultrasonography(next step for diagnosis)Doc A: ... Pancreatic cancer can induce deep vein thrombosis in leg ...      (e.g. ","Ansari et al. 2015)Doc B: ... Deep vein thrombosis is tested by compression ultrasonography ... (e.g. Piovella et al. 2002)
[Tidal Basin, Washington D.C.]The Tidal Basin is a man-made reservoir located between the Potomac River and the Washington Channel in Washington, D.C. It is part of West Potomac Park, is near the National Mall and is a focal point of the National Cherry Blossom Festival held each spring. The Jeﬀerson Memorial, the Martin Luther King Jr. Memorial, the Franklin Delano Roosevelt Memorial, and the George Mason Memorial are situated adjacent to the Tidal Basin. MedQA-USMLE exampleNeed multi-hop reasoning
[The National Cherry Blossom Festival] … It is a spring celebration commemorating the March 27, 1912, gift of Japanese cherry trees from Mayor of Tokyo City to the city of Washington, D.C. ... Of the initial gift of 12 varieties of 3,020 trees, the Yoshino Cherry now dominates. ...Knowledge learned via document linksReference
Question: Roden Brothers were taken over in 1953 by a group headquartered in which Canadian city?Doc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, Canada by Thomas and Frank Roden.  In the 1910s the firm became known as Roden Bros.  Ltd. and were later taken over by Henry Birks and Sons in 1953.  ... In 1974 Roden Bros.  Ltd. published the book, ""Rich Cut Glass"" with Clock House Publications in Peterborough, Ontario, which was a reprint of the 1917 edition published by Roden Bros., Toronto. Doc B: Birks Group (formerly Birks & Mayors) is a designer, manufacturer and retailer of jewellery, timepieces, silverware and gifts, with stores and manufacturing facilities located in Canada and the United States.  As of June 30, 2015, it operates stores under three diﬀerent retail banners: … The company is headquartered in Montreal, Quebec, with American corporate oﬀices located in Tamarac, Florida.LinkBERT prediction: “Montreal” (✓)                                 BERT prediction: “Toronto” (✗)HotpotQA exampleQuestion: Roden Brothers were taken over in 1953 by a group headquartered in which Canadian city?Doc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, Canada by Thomas and Frank Roden.  In the 1910s the firm became known as Roden Bros.  Ltd. and were later taken over by Henry Birks and Sons in 1953.  ... In 1974 Roden Bros.  Ltd. published the book, ""Rich Cut Glass"" with Clock House Publications in Peterborough, Ontario, which was a reprint of the 1917 edition published by Roden Bros., Toronto. Doc B: Birks Group (formerly Birks & Mayors) is a designer, manufacturer and retailer of jewellery, timepieces, silverware and gifts, with stores and manufacturing facilities located in Canada and the United States.  As of June 30, 2015, it operates stores under three different retail banners: ... The company is headquartered in Montreal, Quebec, with American corporate offices located in Tamarac, Florida.LinkBERT prediction: “Montreal” (✓)     BERT prediction: “Toronto” (✗)HotpotQA example
LinkBERT predicts: “Montreal” (✓)      BERT predicts: “Toronto” (✗)Figure 3: Case study of multi-hop reasoning on HotpotQA.
Answering the question needs to identify “Roden Brothers
were taken over by Birks Group” from the ﬁrst document,
and then “Birks Group is headquartered in Montreal” from
the second document. While BERT tends to simply predict
an entity near the question entity (“Toronto” in the ﬁrst
document), LinkBERT correctly predicts the answer in the
second document (“Montreal”).
relevance. In particular, our intuition is that the
DRP objective helps the LM to better recognize
document relations like (anchor document, linked
document) in pret"
Lewis et al. 2020,"al”).
relevance. In particular, our intuition is that the
DRP objective helps the LM to better recognize
document relations like (anchor document, linked
document) in pretraining, which helps to recognize
relations like (question, right document) in down-
stream QA tasks. We indeed ﬁnd that ablating the
DRP objective from LinkBERT hurts performance
(§5.5). The strength of understanding document
relations also suggests the promise of applying
LinkBERT to various retrieval-augmented methods
and tasks (e.g. ","Lewis et al. 2020b), either as the
main LM or the dense retriever component.
Improved few-shot QA performance. We also
ﬁnd that LinkBERT is notably good at few-shot
learning. Concretely, for each MRQA dataset, we
ﬁne-tune with only 10% of the available training
data, and report the performance in Table 4. In this
few-shot regime, LinkBERT attains more signiﬁ-
cant gains over BERT, compared to the full-resource
regime in Table 1 (on NaturalQ, 5.4% vs 1.8% abso-
lute in F1, or 15% vs 7% in relative error re"
Beltagy et al.,"he
ablation result on the DRP objective (§4.2). Re-
moving DRP in pretraining hurts downstream QA
performance. The drop is large on tasks with multi-
ple documents (HotpotQA, TriviaQA, and SQuAD
with distracting documents). This suggests that
DRP facilitates LMs to learn document relations.
6 Biomedical LinkBERT ( BioLinkBERT )
Pretraining LMs on biomedical text is shown
to boost performance on biomedical NLP tasks
(","Beltagy et al., 2019; Lee et al., 2020; Lewis
et al., 2020a; Gu et al., 2020). Biomedical LMs
are typically trained on PubMed, which contains
abstracts and citations of biomedical papers. While
prior works only use their raw text for pretraining,
academic papers have rich dependencies with each
other via citations (references). We hypothesize
that incorporating citation links can help LMs learn
dependencies between p"
 Lee et al.,"The drop is large on tasks with multi-
ple documents (HotpotQA, TriviaQA, and SQuAD
with distracting documents). This suggests that
DRP facilitates LMs to learn document relations.
6 Biomedical LinkBERT ( BioLinkBERT )
Pretraining LMs on biomedical text is shown
to boost performance on biomedical NLP tasks
(Beltagy et al., 2019;"," Lee et al., 2020; Lewis
et al., 2020a; Gu et al., 2020). Biomedical LMs
are typically trained on PubMed, which contains
abstracts and citations of biomedical papers. While
prior works only use their raw text for pretraining,
academic papers have rich dependencies with each
other via citations (references). We hypothesize
that i"
"Lewis
et al.","rformance. The drop is large on tasks with multi-
ple documents (HotpotQA, TriviaQA, and SQuAD
with distracting documents). This suggests that
DRP facilitates LMs to learn document relations.
6 Biomedical LinkBERT ( BioLinkBERT )
Pretraining LMs on biomedical text is shown
to boost performance on biomedical NLP tasks
(Beltagy et al., 2019; Lee et al., 2020; ","Lewis
et al., 2020a; Gu et al., 2020). Biomedical LMs
are typically trained on PubMed, which contains
abstracts and citations of biomedical papers. While
prior works only use their raw text for pretraining,
academic papers have rich dependencies with each
other via citations (references). We hypothesize
that incorporating citation links can help LMs learn
de"
 Gu et al.,"aQA, and SQuAD
with distracting documents). This suggests that
DRP facilitates LMs to learn document relations.
6 Biomedical LinkBERT ( BioLinkBERT )
Pretraining LMs on biomedical text is shown
to boost performance on biomedical NLP tasks
(Beltagy et al., 2019; Lee et al., 2020; Lewis
et al., 2020a;"," Gu et al., 2020). Biomedical LMs
are typically trained on PubMed, which contains
abstracts and citations of biomedical papers. While
prior works only use their raw text for pretraining,
academic papers have rich dependencies with each
other via citations (references). We hypothesize
that incorporat"
Gu et al.," in both domains.LinkBERT consistently improves on baseline LMs
across domains and tasks. For the general domain,
LinkBERT outperforms BERT on MRQA bench-
mark (+4% absolute in F1-score) as well as GLUE
benchmark. For the biomedical domain, LinkBERT
exceeds PubmedBERT (","Gu et al., 2020) and sets
new states of the art on BLURB biomedical NLP
benchmark (+3% absolute in BLURB score) and
MedQA-USMLE reasoning task (+7% absolute in
accuracy). Overall, LinkBERT attains notably large
gains for multi-hop reasoning, multi-document
understanding"
Gu et al.," in both domains.LinkBERT consistently improves on baseline LMs
across domains and tasks. For the general domain,
LinkBERT outperforms BERT on MRQA bench-
mark (+4% absolute in F1-score) as well as GLUE
benchmark. For the biomedical domain, LinkBERT
exceeds PubmedBERT (","Gu et al., 2020) and sets
new states of the art on BLURB biomedical NLP
benchmark (+3% absolute in BLURB score) and
MedQA-USMLE reasoning task (+7% absolute in
accuracy). Overall, LinkBERT attains notably large
gains for multi-hop reasoning, multi-document
understanding"
Gu et al. (2020,"early decay it.
Training took 7 days on eight A100 GPUs with fp16.
Additionally, while the original PubmedBERT
release did not include the -large size, we pretrain
BioLinkBERT of the -large size (340M params)
from scratch, following the same procedure as
-base, except that we use a peak learning rate of 4e-4
and warm up steps of 20%. Training took 21 days
on eight A100 GPUs with fp16.
Baselines. We compare BioLinkBERT with
PubmedBERT released by ","Gu et al. (2020).
6.2 Evaluation tasks
For downstream tasks, we evaluate on the BLURB
benchmark (Gu et al., 2020), a diverse set of biomed-
ical NLP datasets, and MedQA-USMLE (Jin et al.,
2021), a challenging biomedical QA dataset.
BLURB consists of ﬁve named entity recog-
nition tasks, a PICO (population, intervention,
comparison, and outcome) extraction task, three
relation extraction tasks, a sentence similarity task,
a document classiﬁcation "
Gu et al.," in both domains.LinkBERT consistently improves on baseline LMs
across domains and tasks. For the general domain,
LinkBERT outperforms BERT on MRQA bench-
mark (+4% absolute in F1-score) as well as GLUE
benchmark. For the biomedical domain, LinkBERT
exceeds PubmedBERT (","Gu et al., 2020) and sets
new states of the art on BLURB biomedical NLP
benchmark (+3% absolute in BLURB score) and
MedQA-USMLE reasoning task (+7% absolute in
accuracy). Overall, LinkBERT attains notably large
gains for multi-hop reasoning, multi-document
understanding"
Jin et al.,"s of 20%. Training took 21 days
on eight A100 GPUs with fp16.
Baselines. We compare BioLinkBERT with
PubmedBERT released by Gu et al. (2020).
6.2 Evaluation tasks
For downstream tasks, we evaluate on the BLURB
benchmark (Gu et al., 2020), a diverse set of biomed-
ical NLP datasets, and MedQA-USMLE (","Jin et al.,
2021), a challenging biomedical QA dataset.
BLURB consists of ﬁve named entity recog-
nition tasks, a PICO (population, intervention,
comparison, and outcome) extraction task, three
relation extraction tasks, a sentence similarity task,
a document classiﬁcation task, and two question
ans"
Gu et al.," in both domains.LinkBERT consistently improves on baseline LMs
across domains and tasks. For the general domain,
LinkBERT outperforms BERT on MRQA bench-
mark (+4% absolute in F1-score) as well as GLUE
benchmark. For the biomedical domain, LinkBERT
exceeds PubmedBERT (","Gu et al., 2020) and sets
new states of the art on BLURB biomedical NLP
benchmark (+3% absolute in BLURB score) and
MedQA-USMLE reasoning task (+7% absolute in
accuracy). Overall, LinkBERT attains notably large
gains for multi-hop reasoning, multi-document
understanding"
"Jin et al.
(2021","ble 7. We
follow the same ﬁne-tuning method and evaluation
metric used by PubmedBERT (Gu et al., 2020).
MedQA-USMLE is a 4-way multi-choice QA
task that tests biomedical and clinical knowledge.
The questions are from practice tests for the US
Medical License Exams (USMLE). The questions
typically require multi-hop reasoning, e.g., given
patient symptoms, infer the likely cause, and then
answer the appropriate diagnosis procedure (Figure
4). We follow the ﬁne-tuning method in ","Jin et al.
(2021). More details are provided in Appendix B.
MMLU-professional medicine is a multi-
choice QA task that tests biomedical knowledge
and reasoning, and is part of the popular MMLU
5https://github.com/titipata/pubmed_parserPubMed-
BERT baseBioLink-
BERT baseBioLink-
BERT large
Named entity recognition
BC5-chem (Li et al., 2016) 93.33 93.75 94.04
BC5-disease (Li et al., 2016) 85.62 86.10 86.39
NCBI-disease (Do˘gan et al., 2014) 87.82 88.18 88.76
BC2GM (Smith et al."
Li et al.,"x B.
MMLU-professional medicine is a multi-
choice QA task that tests biomedical knowledge
and reasoning, and is part of the popular MMLU
5https://github.com/titipata/pubmed_parserPubMed-
BERT baseBioLink-
BERT baseBioLink-
BERT large
Named entity recognition
BC5-chem (","Li et al., 2016) 93.33 93.75 94.04
BC5-disease (Li et al., 2016) 85.62 86.10 86.39
NCBI-disease (Do˘gan et al., 2014) 87.82 88.18 88.76
BC2GM (Smith et al., 2008) 84.52 84.90 85.18
JNLPBA (Kim et al., 2004) 80.06 79.03 80.06
PICO extraction
EBM PICO (Nye et al., 2018) 7"
Li et al.,"x B.
MMLU-professional medicine is a multi-
choice QA task that tests biomedical knowledge
and reasoning, and is part of the popular MMLU
5https://github.com/titipata/pubmed_parserPubMed-
BERT baseBioLink-
BERT baseBioLink-
BERT large
Named entity recognition
BC5-chem (","Li et al., 2016) 93.33 93.75 94.04
BC5-disease (Li et al., 2016) 85.62 86.10 86.39
NCBI-disease (Do˘gan et al., 2014) 87.82 88.18 88.76
BC2GM (Smith et al., 2008) 84.52 84.90 85.18
JNLPBA (Kim et al., 2004) 80.06 79.03 80.06
PICO extraction
EBM PICO (Nye et al., 2018) 7"
Smith et al.," task that tests biomedical knowledge
and reasoning, and is part of the popular MMLU
5https://github.com/titipata/pubmed_parserPubMed-
BERT baseBioLink-
BERT baseBioLink-
BERT large
Named entity recognition
BC5-chem (Li et al., 2016) 93.33 93.75 94.04
BC5-disease (Li et al., 2016) 85.62 86.10 86.39
NCBI-disease (Do˘gan et al., 2014) 87.82 88.18 88.76
BC2GM (","Smith et al., 2008) 84.52 84.90 85.18
JNLPBA (Kim et al., 2004) 80.06 79.03 80.06
PICO extraction
EBM PICO (Nye et al., 2018) 73.38 73.97 74.19
Relation extraction
ChemProt (Krallinger et al., 2017) 77.24 77.57 79.98
DDI (Herrero-Zazo et al., 2013) 82.36 82.72 83.35
GAD (Bravo et al., 2015) 82.34 84.39 84.90
Sentence similarity
BIOSSES (So˘gancıo ˘glu et al."
Kim et al.,"itipata/pubmed_parserPubMed-
BERT baseBioLink-
BERT baseBioLink-
BERT large
Named entity recognition
BC5-chem (Li et al., 2016) 93.33 93.75 94.04
BC5-disease (Li et al., 2016) 85.62 86.10 86.39
NCBI-disease (Do˘gan et al., 2014) 87.82 88.18 88.76
BC2GM (Smith et al., 2008) 84.52 84.90 85.18
JNLPBA (","Kim et al., 2004) 80.06 79.03 80.06
PICO extraction
EBM PICO (Nye et al., 2018) 73.38 73.97 74.19
Relation extraction
ChemProt (Krallinger et al., 2017) 77.24 77.57 79.98
DDI (Herrero-Zazo et al., 2013) 82.36 82.72 83.35
GAD (Bravo et al., 2015) 82.34 84.39 84.90
Sentence similarity
BIOSSES (So˘ganc"
Nye et al.,"k-
BERT large
Named entity recognition
BC5-chem (Li et al., 2016) 93.33 93.75 94.04
BC5-disease (Li et al., 2016) 85.62 86.10 86.39
NCBI-disease (Do˘gan et al., 2014) 87.82 88.18 88.76
BC2GM (Smith et al., 2008) 84.52 84.90 85.18
JNLPBA (Kim et al., 2004) 80.06 79.03 80.06
PICO extraction
EBM PICO (","Nye et al., 2018) 73.38 73.97 74.19
Relation extraction
ChemProt (Krallinger et al., 2017) 77.24 77.57 79.98
DDI (Herrero-Zazo et al., 2013) 82.36 82.72 83.35
GAD (Bravo et al., 2015) 82.34 84.39 84.90
Sentence similarity
BIOSSES (So˘gancıo ˘glu et al., 2017) 92.30 93.25 93.63
Document classiﬁcation"
Krallinger et al.,"cal knowledge
and reasoning, and is part of the popular MMLU
5https://github.com/titipata/pubmed_parserPubMed-
BERT baseBioLink-
BERT baseBioLink-
BERT large
Named entity recognition
BC5-chem (Li et al., 2016) 93.33 93.75 94.04
BC5-disease (Li et al., 2016) 85.62 86.10 86.39
NCBI-disease (Do˘gan et al., 2014) 87.82 88.18 88.76
BC2GM (Smith et al., 2008) 84.52 84.90 85.18
JNLPBA (Kim et al., 2004) 80.06 79.03 80.06
PICO extraction
EBM PICO (Nye et al., 2018) 73.38 73.97 74.19
Relation extraction
ChemProt (","Krallinger et al., 2017) 77.24 77.57 79.98
DDI (Herrero-Zazo et al., 2013) 82.36 82.72 83.35
GAD (Bravo et al., 2015) 82.34 84.39 84.90
Sentence similarity
BIOSSES (So˘gancıo ˘glu et al., 2017) 92.30 93.25 93.63
Document classiﬁcation
HoC (Baker et al., 2016) 82.32 84.35 84.87
Question answering
PubMedQA (Jin et al., 2019) 55.84 70.20 72.18
BioASQ (Nentidis et al., 2019) 87.56 91.43 94.82
BLURB score 81.10 83.39 84.30
Table 7: Performance on BLURB benchmark. BioLinkBERT
attains improvement on all tasks, e"
Herrero-Zazo et al.,"ests biomedical knowledge
and reasoning, and is part of the popular MMLU
5https://github.com/titipata/pubmed_parserPubMed-
BERT baseBioLink-
BERT baseBioLink-
BERT large
Named entity recognition
BC5-chem (Li et al., 2016) 93.33 93.75 94.04
BC5-disease (Li et al., 2016) 85.62 86.10 86.39
NCBI-disease (Do˘gan et al., 2014) 87.82 88.18 88.76
BC2GM (Smith et al., 2008) 84.52 84.90 85.18
JNLPBA (Kim et al., 2004) 80.06 79.03 80.06
PICO extraction
EBM PICO (Nye et al., 2018) 73.38 73.97 74.19
Relation extraction
ChemProt (Krallinger et al., 2017) 77.24 77.57 79.98
DDI (","Herrero-Zazo et al., 2013) 82.36 82.72 83.35
GAD (Bravo et al., 2015) 82.34 84.39 84.90
Sentence similarity
BIOSSES (So˘gancıo ˘glu et al., 2017) 92.30 93.25 93.63
Document classiﬁcation
HoC (Baker et al., 2016) 82.32 84.35 84.87
Question answering
PubMedQA (Jin et al., 2019) 55.84 70.20 72.18
BioASQ (Nentidis et al., 2019) 87.56 91.43 94.82
BLURB score 81.10 83.39 84.30
Table 7: Performance on BLURB benchmark. BioLinkBERT
attains improvement on all tasks, establishing new state of
the art on BLURB. Gains are notably large on document-level
tasks such as PubMedQA "
Bravo et al.,"l., 2016) 85.62 86.10 86.39
NCBI-disease (Do˘gan et al., 2014) 87.82 88.18 88.76
BC2GM (Smith et al., 2008) 84.52 84.90 85.18
JNLPBA (Kim et al., 2004) 80.06 79.03 80.06
PICO extraction
EBM PICO (Nye et al., 2018) 73.38 73.97 74.19
Relation extraction
ChemProt (Krallinger et al., 2017) 77.24 77.57 79.98
DDI (Herrero-Zazo et al., 2013) 82.36 82.72 83.35
GAD (","Bravo et al., 2015) 82.34 84.39 84.90
Sentence similarity
BIOSSES (So˘gancıo ˘glu et al., 2017) 92.30 93.25 93.63
Document classiﬁcation
HoC (Baker et al., 2016) 82.32 84.35 84.87
Question answering
PubMedQA (Jin et al., 2019) 55.84 70.20 72.18
BioASQ (Nentidis et al., 2019) 87.56 91.43 94.82
BLURB score 81.10 83.39 84.30
Table 7: Performance on BLURB benchm"
Baker et al.,"l., 2004) 80.06 79.03 80.06
PICO extraction
EBM PICO (Nye et al., 2018) 73.38 73.97 74.19
Relation extraction
ChemProt (Krallinger et al., 2017) 77.24 77.57 79.98
DDI (Herrero-Zazo et al., 2013) 82.36 82.72 83.35
GAD (Bravo et al., 2015) 82.34 84.39 84.90
Sentence similarity
BIOSSES (So˘gancıo ˘glu et al., 2017) 92.30 93.25 93.63
Document classiﬁcation
HoC (","Baker et al., 2016) 82.32 84.35 84.87
Question answering
PubMedQA (Jin et al., 2019) 55.84 70.20 72.18
BioASQ (Nentidis et al., 2019) 87.56 91.43 94.82
BLURB score 81.10 83.39 84.30
Table 7: Performance on BLURB benchmark. BioLinkBERT
attains improvement on all tasks, establishing new state of
the art on BLURB. Gains are notably large on document-level
tasks"
Jin et al.,"s of 20%. Training took 21 days
on eight A100 GPUs with fp16.
Baselines. We compare BioLinkBERT with
PubmedBERT released by Gu et al. (2020).
6.2 Evaluation tasks
For downstream tasks, we evaluate on the BLURB
benchmark (Gu et al., 2020), a diverse set of biomed-
ical NLP datasets, and MedQA-USMLE (","Jin et al.,
2021), a challenging biomedical QA dataset.
BLURB consists of ﬁve named entity recog-
nition tasks, a PICO (population, intervention,
comparison, and outcome) extraction task, three
relation extraction tasks, a sentence similarity task,
a document classiﬁcation task, and two question
ans"
Nentidis et al.," 80.06
PICO extraction
EBM PICO (Nye et al., 2018) 73.38 73.97 74.19
Relation extraction
ChemProt (Krallinger et al., 2017) 77.24 77.57 79.98
DDI (Herrero-Zazo et al., 2013) 82.36 82.72 83.35
GAD (Bravo et al., 2015) 82.34 84.39 84.90
Sentence similarity
BIOSSES (So˘gancıo ˘glu et al., 2017) 92.30 93.25 93.63
Document classiﬁcation
HoC (Baker et al., 2016) 82.32 84.35 84.87
Question answering
PubMedQA (Jin et al., 2019) 55.84 70.20 72.18
BioASQ (","Nentidis et al., 2019) 87.56 91.43 94.82
BLURB score 81.10 83.39 84.30
Table 7: Performance on BLURB benchmark. BioLinkBERT
attains improvement on all tasks, establishing new state of
the art on BLURB. Gains are notably large on document-level
tasks such as PubMedQA and BioASQ.
Methods Acc. (%)
BioBERT large (Lee et al., 2020) 36.7
QAGNN (Yasunaga et al., 2021) 38.0
GreaseLM (Zhang et al., 2022) 38.5
PubmedBERT base (Gu et al., 2020) 38.1
BioLink"
Lee et al.," multi-
ple documents (HotpotQA, TriviaQA, and SQuAD
with distracting documents). This suggests that
DRP facilitates LMs to learn document relations.
6 Biomedical LinkBERT ( BioLinkBERT )
Pretraining LMs on biomedical text is shown
to boost performance on biomedical NLP tasks
(Beltagy et al., 2019; ","Lee et al., 2020; Lewis
et al., 2020a; Gu et al., 2020). Biomedical LMs
are typically trained on PubMed, which contains
abstracts and citations of biomedical papers. While
prior works only use their raw text for pretraining,
academic papers have rich dependencies with each
other via citations (refer"
Yasunaga et al.,"uestion answering.
Ma et al. (2021) study various hyperlink-aware
pretraining tasks for retrieval. While these works
use hyperlinks to learn retrievers, we focus on using
hyperlinks to create better context for learning
general-purpose LMs. Separately, Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; ","Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasu"
Zhang et al.,"ers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. ","Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasunaga et al.
(2021); Zhang et al. (2022) combine LMs and graph
neural networks (GNNs) to jointly train on text and
KGs. Different from KGs, we use document graphs
to learn knowledge that spans across documents.
3 Preliminaries
A language m"
Gu et al.," in both domains.LinkBERT consistently improves on baseline LMs
across domains and tasks. For the general domain,
LinkBERT outperforms BERT on MRQA bench-
mark (+4% absolute in F1-score) as well as GLUE
benchmark. For the biomedical domain, LinkBERT
exceeds PubmedBERT (","Gu et al., 2020) and sets
new states of the art on BLURB biomedical NLP
benchmark (+3% absolute in BLURB score) and
MedQA-USMLE reasoning task (+7% absolute in
accuracy). Overall, LinkBERT attains notably large
gains for multi-hop reasoning, multi-document
understanding"
Brown et al.,"asoning and
few-shot QA (+5% absolute improvement on
HotpotQA and TriviaQA), and our biomedical
LinkBERT sets new states of the art on various
BioNLP tasks (+7% on BioASQ and USMLE).
We release our pretrained models, LinkBERT
andBioLinkBERT , as well as code and data.1
1 Introduction
Pretrained language models (LMs), like BERT and
GPTs (Devlin et al., 2019; ","Brown et al., 2020), have
shown remarkable performance on many natural
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (Devlin
et al., 2019), LMs learn to encode various knowl-
"
Khashabi et al.,"document-level
tasks such as PubMedQA and BioASQ.
Methods Acc. (%)
BioBERT large (Lee et al., 2020) 36.7
QAGNN (Yasunaga et al., 2021) 38.0
GreaseLM (Zhang et al., 2022) 38.5
PubmedBERT base (Gu et al., 2020) 38.1
BioLinkBERT base(Ours ) 40.0
BioLinkBERT large (Ours ) 44.6
Table 8: Performance on MedQA-USMLE. BioLinkBERT
outperforms all previous biomedical LMs.
Methods Acc. (%)
GPT-3 (175B params) (Brown et al., 2020) 38.7
UniﬁedQA (11B params) (","Khashabi et al., 2020) 43.2
BioLinkBERT large (Ours ) 50.7
Table 9: Performance on MMLU-professional medicine.
BioLinkBERT signiﬁcantly outperforms the largest general-
domain LM or QA model, despite having just 340M parameters.
benchmark (Hendrycks et al., 2021) that is used
to evaluate massive language models. We take the
BioLinkBERT ﬁne-tuned on the above MedQA-
USMLE task, and evaluate on this task without
further adaptation.
6.3 Results
BLUR"
Hendrycks et al.,"8.1
BioLinkBERT base(Ours ) 40.0
BioLinkBERT large (Ours ) 44.6
Table 8: Performance on MedQA-USMLE. BioLinkBERT
outperforms all previous biomedical LMs.
Methods Acc. (%)
GPT-3 (175B params) (Brown et al., 2020) 38.7
UniﬁedQA (11B params) (Khashabi et al., 2020) 43.2
BioLinkBERT large (Ours ) 50.7
Table 9: Performance on MMLU-professional medicine.
BioLinkBERT signiﬁcantly outperforms the largest general-
domain LM or QA model, despite having just 340M parameters.
benchmark (","Hendrycks et al., 2021) that is used
to evaluate massive language models. We take the
BioLinkBERT ﬁne-tuned on the above MedQA-
USMLE task, and evaluate on this task without
further adaptation.
6.3 Results
BLURB. Table 7 shows the results on BLURB.
BioLinkBERT baseoutperforms PubmedBERT base
on all task categories, attaining a performance
boost of +2% absolute on average. Moreover,
BioLinkBERT large provides a further boost of
+1%. In total, BioLinkBERT outperforms the
previo"
Ansari et al. 2015)Doc B: ... Deep vein thrombosis is tested by compression ultrasonography ... (e.g. Piovella et al. 2002,"(§4.2).
et al., 2007), and report the average score. More
ﬁne-tuning details are provided in Appendix B.
5.3 Results
Table 1 shows the performance (F1 score) on
MRQA datasets. LinkBERT substantially outper-
forms BERT on all datasets. On average, the gain is
+4.1% absolute for the BERT tinyscale, +2.6% for
the BERT basescale, and +2.5% for the BERT large
scale. Table 2 shows the results on GLUE, where
LinkBERT performs moderately better than BERT.
These results suggest that LinkBERT is especially
effective at learning knowledge useful for QA tasks
(e.g. world knowledge), while keeping performance
on sentence-level language understanding.
5.4 Analysis
We further study when LinkBERT is especially
useful in downstream tasks.Improved multi-hop reasoning. In Table 1,
we ﬁnd that LinkBERT obtains notably large
gains on QA datasets that require reasoning with
multiple documents, such as HotpotQA (+5% over
BERT tiny), TriviaQA (+6%) and SearchQA (+8%),
as opposed to SQuAD (+1.4%) which just has
a single document per question. To further gain
qualitative insights, we studied in what QA exam-
ples LinkBERT succeeds but BERT fails. Figure
3 shows a representative example from HotpotQA.
Answering the question needs 2-hop reasoning:
identify “Roden Brothers were taken over by Birks
Group” from the ﬁrst document, and then “Birks
Group is headquartered in Montreal” from the sec-
ond document. While BERT tends to simply predict
an entity near the question entity (“Toronto” in the
ﬁrst document, which is just 1-hop), LinkBERT
correctly predicts the answer in the second docu-
ment (“Montreal”). Our intuition is that because
LinkBERT is pretrained with pairs of linked docu-
ments rather than purely single documents, it better
learns how to ﬂow information (e.g., do attention)
across tokens when multiple related documents
are given in the context. In summary, these results
suggest that pretraining with linked documents
helps for multi-hop reasoning on downstream tasks.
Improved understanding of document relations.
While the MRQA datasets typically use ground-
truth documents as context for answering questions,
in open-domain QA, QA systems need to use
documents obtained by a retriever, which may
include noisy documents besides gold ones (Chen
et al., 2017; Dunn et al., 2017). In such cases, QA
systems need to understand the document relations
to perform well (Yang et al., 2018). To simulate
this setting, we modify the SQuAD dataset by
prepending or appending 1–2 distracting documents
to the original document given to each question.
Table 3 shows the result. While BERT incurs a large
performance drop (-2.8%), LinkBERT is robust to
distracting documents (-0.5%). This result suggests
that pretraining with document links improves
the ability to understand document relations andThree days after undergoing a laparoscopic Whipple's procedure, a 43-year-old woman has swelling of her right leg. ... She was diagnosed with pancreatic cancer 1 month ago. ... Her temperature is 38°C (100.4°F), pulse is 90/min, and blood pressure is 118/78 mm Hg. Examination shows mild swelling of the right thigh to the ankle; there is no erythema or pitting edema. ... Which of the following is the most appropriate next step in management?(A)  CT pulmonary angiography     (B)  Compression ultrasonography(C)  D-dimer level                                 (D)  2 sets of blood culturesLinkBERT predicts: B (✓)    PubmedBERT predicts: D (✗)Leg swelling, pancreatic cancer(symptom) Deep vein thrombosis(possible cause)Compression ultrasonography(next step for diagnosis)Doc A: ... Pancreatic cancer can induce deep vein thrombosis in leg ...      (e.g. ","Ansari et al. 2015)Doc B: ... Deep vein thrombosis is tested by compression ultrasonography ... (e.g. Piovella et al. 2002)
[Tidal Basin, Washington D.C.]The Tidal Basin is a man-made reservoir located between the Potomac River and the Washington Channel in Washington, D.C. It is part of West Potomac Park, is near the National Mall and is a focal point of the National Cherry Blossom Festival held each spring. The Jeﬀerson Memorial, the Martin Luther King Jr. Memorial, the Franklin Delano Roosevelt Memorial, and the George Mason Memorial are situated adjacent to the Tidal Basin. MedQA-USMLE exampleNeed multi-hop reasoning
[The National Cherry Blossom Festival] … It is a spring celebration commemorating the March 27, 1912, gift of Japanese cherry trees from Mayor of Tokyo City to the city of Washington, D.C. ... Of the initial gift of 12 varieties of 3,020 trees, the Yoshino Cherry now dominates. ...Knowledge learned via document linksReference
Question: Roden Brothers were taken over in 1953 by a group headquartered in which Canadian city?Doc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, Canada by Thomas and Frank Roden.  In the 1910s the firm became known as Roden Bros.  Ltd. and were later taken over by Henry Birks and Sons in 1953.  ... In 1974 Roden Bros.  Ltd. published the book, ""Rich Cut Glass"" with Clock House Publications in Peterborough, Ontario, which was a reprint of the 1917 edition published by Roden Bros., Toronto. Doc B: Birks Group (formerly Birks & Mayors) is a designer, manufacturer and retailer of jewellery, timepieces, silverware and gifts, with stores and manufacturing facilities located in Canada and the United States.  As of June 30, 2015, it operates stores under three diﬀerent retail banners: … The company is headquartered in Montreal, Quebec, with American corporate oﬀices located in Tamarac, Florida.LinkBERT prediction: “Montreal” (✓)                                 BERT prediction: “Toronto” (✗)HotpotQA exampleQuestion: Roden Brothers were taken over in 1953 by a group headquartered in which Canadian city?Doc A: Roden Brothers was founded June 1, 1891 in Toronto, Ontario, Canada by Thomas and Frank Roden.  In the 1910s the firm became known as Roden Bros.  Ltd. and were later taken over by Henry Birks and Sons in 1953.  ... In 1974 Roden Bros.  Ltd. published the book, ""Rich Cut Glass"" with Clock House Publications in Peterborough, Ontario, which was a reprint of the 1917 edition published by Roden Bros., Toronto. Doc B: Birks Group (formerly Birks & Mayors) is a designer, manufacturer and retailer of jewellery, timepieces, silverware and gifts, with stores and manufacturing facilities located in Canada and the United States.  As of June 30, 2015, it operates stores under three different retail banners: ... The company is headquartered in Montreal, Quebec, with American corporate offices located in Tamarac, Florida.LinkBERT prediction: “Montreal” (✓)     BERT prediction: “Toronto” (✗)HotpotQA example
LinkBERT predicts: “Montreal” (✓)      BERT predicts: “Toronto” (✗)Figure 3: Case study of multi-hop reasoning on HotpotQA.
Answering the question needs to identify “Roden Brothers
were taken over by Birks Group” from the ﬁrst document,
and then “Birks Group is headquartered in Montreal” from
the second document. While BERT tends to simply predict
an entity near the question entity (“Toronto” in the ﬁrst
document), LinkBERT correctly predicts the answer in the
second document (“Montreal”).
relevance. In particular, our intuition is that the
DRP objective helps the LM to better recognize
document relations like (anchor document, linked
document) in pret"
Ansari et al. (2015,"“blood” for choice D), BioLinkBERT
correctly predicts the answer (B). Our intuition is
that citation links bring relevant documents and
concepts together in the same context in pretraining
(right),6which readily provides the multi-hop
knowledge needed for the reasoning (center). Com-
bined with the analysis on HotpotQA (§5.4), our
results suggest that pretraining with document links
consistently helps for multi-hop reasoning across
domains (e.g., general documents with hyperlinks
and biomedical articles with citation links).
6For instance, as in Figure 4 (right), ","Ansari et al. (2015) in
PubMed mention that pancreatic cancer can induce deep vein
thrombosis in leg , and it cites a paper in PubMed, Piovella et al.
(2002), which mention that deep vein thrombosis is tested by
compression ultrasonography . Placing these two documents
in the same context yields the complete multi-hop knowledge
needed to answer the question (“ pancreatic cancer ”!“deep
vein thrombosis ”!“compression ultrasonography ”).MMLU-professional medicine. Table 9 shows
the performance. Despite having just 340M parame-
ters, BioLinkBERT large achieves 50% ac"
"Piovella et al.
(2002","ntuition is
that citation links bring relevant documents and
concepts together in the same context in pretraining
(right),6which readily provides the multi-hop
knowledge needed for the reasoning (center). Com-
bined with the analysis on HotpotQA (§5.4), our
results suggest that pretraining with document links
consistently helps for multi-hop reasoning across
domains (e.g., general documents with hyperlinks
and biomedical articles with citation links).
6For instance, as in Figure 4 (right), Ansari et al. (2015) in
PubMed mention that pancreatic cancer can induce deep vein
thrombosis in leg , and it cites a paper in PubMed, ","Piovella et al.
(2002), which mention that deep vein thrombosis is tested by
compression ultrasonography . Placing these two documents
in the same context yields the complete multi-hop knowledge
needed to answer the question (“ pancreatic cancer ”!“deep
vein thrombosis ”!“compression ultrasonography ”).MMLU-professional medicine. Table 9 shows
the performance. Despite having just 340M parame-
ters, BioLinkBERT large achieves 50% accuracy on
this QA task, signiﬁcantly outperforming the largest
general-domain LM or QA models such as GPT-3
175B params (39% accuracy) and UniﬁedQA 11B
params (43% accuracy). This result shows th"
Sheng et al.,"ions and risks
We outline potential ethical issues with our work
below. First, LinkBERT is trained on the same
text corpora (e.g., Wikipedia, Books, PubMed)
as in existing language models. Consequently,
LinkBERT could reﬂect the same biases and toxic
behaviors exhibited by language models, such as
biases about race, gender, and other demographic
attributes (","Sheng et al., 2020).
Another source of ethical concern is the use of
the MedQA-USMLE evaluation (Jin et al., 2021).
While we ﬁnd this clinical reasoning task to be an
interesting testbed for LinkBERT and for multi-hop
reasoning in general, we do not encourage users
to use the current models for real world clinical
prediction.
B Fine-tuning details
We apply t"
Jin et al.,"s of 20%. Training took 21 days
on eight A100 GPUs with fp16.
Baselines. We compare BioLinkBERT with
PubmedBERT released by Gu et al. (2020).
6.2 Evaluation tasks
For downstream tasks, we evaluate on the BLURB
benchmark (Gu et al., 2020), a diverse set of biomed-
ical NLP datasets, and MedQA-USMLE (","Jin et al.,
2021), a challenging biomedical QA dataset.
BLURB consists of ﬁve named entity recog-
nition tasks, a PICO (population, intervention,
comparison, and outcome) extraction task, three
relation extraction tasks, a sentence similarity task,
a document classiﬁcation task, and two question
ans"
