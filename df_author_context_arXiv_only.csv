context_left,context_right,DOI,Abstract
"ur biomedical
LinkBERT sets new states of the art on various
BioNLP tasks (+7% on BioASQ and USMLE).
We release our pretrained models, LinkBERT
andBioLinkBERT , as well as code and data.1
1 Introduction
Pretrained language models (LMs), like BERT and
GPTs (Devlin et al., 2019; Brown et al., 2020), have
shown remarkable performance on many natural
language processing (NLP) tasks, such as text
classiﬁcation and question answering, becoming the
foundation of modern NLP systems (","Bommasani
et al., 2021). By performing self-supervised learn-
ing, such as masked language modeling (Devlin
et al., 2019), LMs learn to encode various knowl-
edge from text corpora and produce informative
representations for downstream tasks (Petroni et al.,
2019; Bosselut et al., 2019; Raffel et al., 2020).
Equal senior authorship.
1Available at https://github.com/michiyasunaga/
LinkBERT .
Language Model[CLS] The Tidal Basin [SEP] [MASK] [MASK] trees... ... [SEP] Segment AS",2108.07258,"AI is undergoing a paradigm shift with the rise of models (e.g., BERT,
DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a
wide range of downstream tasks. We call these models foundation models to
underscore their critically central yet incomplete character. This report
provides a thorough account of the opportunities and risks of foundation
models, ranging from their capabilities (e.g., language, vision, robotics,
reasoning, human interaction) and technical principles(e.g., model
architectures, training procedures, data, systems, security, evaluation,
theory) to their applications (e.g., law, healthcare, education) and societal
impact (e.g., inequity, misuse, economic and environmental impact, legal and
ethical considerations). Though foundation models are based on standard deep
learning and transfer learning, their scale results in new emergent
capabilities,and their effectiveness across so many tasks incentivizes
homogenization. Homogenization provides powerful leverage but demands caution,
as the defects of the foundation model are inherited by all the adapted models
downstream. Despite the impending widespread deployment of foundation models,
we currently lack a clear understanding of how they work, when they fail, and
what they are even capable of due to their emergent properties. To tackle these
questions, we believe much of the critical research on foundation models will
require deep interdisciplinary collaboration commensurate with their
fundamentally sociotechnical nature."
"us applications, including
answering a question “What trees can you see at Tidal Basin?”.
We aim to leverage document links to incorporate more
knowledge into language model pretraining.
However, existing LM pretraining methods typ-
ically consider text from a single document in each
input context (","Liu et al., 2019; Joshi et al., 2020)
and do not model links between documents. This
can pose limitations because documents often have
rich dependencies (e.g. hyperlinks, references), and
knowledge can span across documents. As an exam-
ple, in Figure 1, the Wikipedia article “Tidal Basin,
Washingto",1907.11692,"Language model pretraining has led to significant performance gains but
careful comparison between different approaches is challenging. Training is
computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the
final results. We present a replication study of BERT pretraining (Devlin et
al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can
match or exceed the performance of every model published after it. Our best
model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results
highlight the importance of previously overlooked design choices, and raise
questions about the source of recently reported improvements. We release our
models and code."
" in both domains.LinkBERT consistently improves on baseline LMs
across domains and tasks. For the general domain,
LinkBERT outperforms BERT on MRQA bench-
mark (+4% absolute in F1-score) as well as GLUE
benchmark. For the biomedical domain, LinkBERT
exceeds PubmedBERT (","Gu et al., 2020) and sets
new states of the art on BLURB biomedical NLP
benchmark (+3% absolute in BLURB score) and
MedQA-USMLE reasoning task (+7% absolute in
accuracy). Overall, LinkBERT attains notably large
gains for multi-hop reasoning, multi-document
understanding",2007.15779,"Pretraining large neural language models, such as BERT, has led to impressive
gains on many natural language processing (NLP) tasks. However, most
pretraining efforts focus on general domain corpora, such as newswire and Web.
A prevailing assumption is that even domain-specific pretraining can benefit by
starting from general-domain language models. In this paper, we challenge this
assumption by showing that for domains with abundant unlabeled text, such as
biomedicine, pretraining language models from scratch results in substantial
gains over continual pretraining of general-domain language models. To
facilitate this investigation, we compile a comprehensive biomedical NLP
benchmark from publicly-available datasets. Our experiments show that
domain-specific pretraining serves as a solid foundation for a wide range of
biomedical NLP tasks, leading to new state-of-the-art results across the board.
Further, in conducting a thorough evaluation of modeling choices, both for
pretraining and task-specific fine-tuning, we discover that some common
practices are unnecessary with BERT models, such as using complex tagging
schemes in named entity recognition (NER). To help accelerate research in
biomedical NLP, we have released our state-of-the-art pretrained and
task-specific models for the community, and created a leaderboard featuring our
BLURB benchmark (short for Biomedical Language Understanding & Reasoning
Benchmark) at https://aka.ms/BLURB."
"arge
gains for multi-hop reasoning, multi-document
understanding, and few-shot question answering,
suggesting that LinkBERT internalizes signiﬁcantly
more knowledge than existing LMs by pretraining
with document link information.
2 Related work
Retrieval-augmented LMs. Several works
(Lewis et al., 2020b; Karpukhin et al., 2020; ","Oguz
et al., 2020; Xie et al., 2022) introduce a retrieval
module for LMs, where given an anchor text
(e.g. question), retrieved text is added to the same
LM context to improve model inference (e.g. an-
swer prediction). These works show the promise of
placing related documents in the same LM context
at inference time, but they ",2012.14610,"We study open-domain question answering with structured, unstructured and
semi-structured knowledge sources, including text, tables, lists and knowledge
bases. Departing from prior work, we propose a unifying approach that
homogenizes all sources by reducing them to text and applies the
retriever-reader model which has so far been limited to text sources only. Our
approach greatly improves the results on knowledge-base QA tasks by 11 points,
compared to latest graph-based methods. More importantly, we demonstrate that
our unified knowledge (UniK-QA) model is a simple and yet effective way to
combine heterogeneous sources of knowledge, advancing the state-of-the-art
results on two popular question answering benchmarks, NaturalQuestions and
WebQuestions, by 3.5 and 2.6 points, respectively.
  The code of UniK-QA is available at:
https://github.com/facebookresearch/UniK-QA."
"ti-hop reasoning, multi-document
understanding, and few-shot question answering,
suggesting that LinkBERT internalizes signiﬁcantly
more knowledge than existing LMs by pretraining
with document link information.
2 Related work
Retrieval-augmented LMs. Several works
(Lewis et al., 2020b; Karpukhin et al., 2020; Oguz
et al., 2020;"," Xie et al., 2022) introduce a retrieval
module for LMs, where given an anchor text
(e.g. question), retrieved text is added to the same
LM context to improve model inference (e.g. an-
swer prediction). These works show the promise of
placing related documents in the same LM context
at inference time, but they do not study the e",2201.05966,"Structured knowledge grounding (SKG) leverages structured knowledge to
complete user requests, such as semantic parsing over databases and question
answering over knowledge bases. Since the inputs and outputs of SKG tasks are
heterogeneous, they have been studied separately by different communities,
which limits systematic and compatible research on SKG. In this paper, we
overcome this limitation by proposing the UnifiedSKG framework, which unifies
21 SKG tasks into a text-to-text format, aiming to promote systematic SKG
research, instead of being exclusive to a single task, domain, or dataset. We
use UnifiedSKG to benchmark T5 with different sizes and show that T5, with
simple modifications when necessary, achieves state-of-the-art performance on
almost all of the 21 tasks. We further demonstrate that multi-task
prefix-tuning improves the performance on most tasks, largely improving the
overall performance. UnifiedSKG also facilitates the investigation of zero-shot
and few-shot learning, and we show that T0, GPT-3, and Codex struggle in
zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a
series of controlled experiments on structured knowledge encoding variants
across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is
open-sourced at https://github.com/hkunlp/unifiedskg."
" LM with a retriever that learns to retrieve text for
answering masked tokens in the anchor text. In con-
trast, our focus is not on retrieval, but on pretraining
a general-purpose LM that internalizes knowledge
that spans across documents, which is orthogonal
to the above works (e.g., our pretrained LM could
be used to initialize the LM component of these
works). Additionally, we focus on incorporating
document links such as hyperlinks, which can offer
salient knowledge that common lexical retrieval
methods may not provide (Asai et al., 2020).
Pretrain LMs with related documents. Several
concurrent works use multiple related documentsto pretrain LMs. ","Caciularu et al. (2021) place doc-
uments (news articles) about the same topic into the
same LM context, and Levine et al. (2021) place sen-
tences of high lexical similarity into the same con-
text. Our work provides a general method to incor-
porate document links into LM pretraining, where
lexical or topical similarity can be one instance of
document links, besides hyperlinks. We focus on hy-
perlinks in this work, because we ﬁnd they can bring
in salient knowledge that may not be obvious via
lexical similarity, and yield a more performant LM
(§5.5). Additionally, we propose the DRP objective,
which improves modeling multiple documents and
relations",2101.00406,"We introduce a new pretraining approach geared for multi-document language
modeling, incorporating two key ideas into the masked language modeling
self-supervised objective. First, instead of considering documents in
isolation, we pretrain over sets of multiple related documents, encouraging the
model to learn cross-document relationships. Second, we improve over recent
long-range transformers by introducing dynamic global attention that has access
to the entire input to predict masked tokens. We release CDLM (Cross-Document
Language Model), a new general language model for multi-document setting that
can be easily applied to downstream tasks. Our extensive analysis shows that
both ideas are essential for the success of CDLM, and work in synergy to set
new state-of-the-art results for several multi-text tasks. Code and models are
available at https://github.com/aviclu/CDLM."
"es knowledge
that spans across documents, which is orthogonal
to the above works (e.g., our pretrained LM could
be used to initialize the LM component of these
works). Additionally, we focus on incorporating
document links such as hyperlinks, which can offer
salient knowledge that common lexical retrieval
methods may not provide (Asai et al., 2020).
Pretrain LMs with related documents. Several
concurrent works use multiple related documentsto pretrain LMs. Caciularu et al. (2021) place doc-
uments (news articles) about the same topic into the
same LM context, and ","Levine et al. (2021) place sen-
tences of high lexical similarity into the same con-
text. Our work provides a general method to incor-
porate document links into LM pretraining, where
lexical or topical similarity can be one instance of
document links, besides hyperlinks. We focus on hy-
perlinks in this work, because we ﬁnd they can bring
in salient knowledge that may not be obvious via
lexical similarity, and yield a more performant LM
(§5.5). Additionally, we propose the DRP objective,
which improves modeling multiple documents and
relations between them in LM",2110.04541,"Pretraining Neural Language Models (NLMs) over a large corpus involves
chunking the text into training examples, which are contiguous text segments of
sizes processable by the neural architecture. We highlight a bias introduced by
this common practice: we prove that the pretrained NLM can model much stronger
dependencies between text segments that appeared in the same training example,
than it can between text segments that appeared in different training examples.
This intuitive result has a twofold role. First, it formalizes the motivation
behind a broad line of recent successful NLM training heuristics, proposed for
the pretraining and fine-tuning stages, which do not necessarily appear related
at first glance. Second, our result clearly indicates further improvements to
be made in NLM pretraining for the benefit of Natural Language Understanding
tasks. As an example, we propose ""kNN-Pretraining"": we show that including
semantically related non-neighboring sentences in the same pretraining example
yields improved sentence representations and open domain question answering
abilities. This theoretically motivated degree of freedom for pretraining
example design indicates new training schemes for self-improving
representations."
"uestion answering.
Ma et al. (2021) study various hyperlink-aware
pretraining tasks for retrieval. While these works
use hyperlinks to learn retrievers, we focus on using
hyperlinks to create better context for learning
general-purpose LMs. Separately, Calixto et al.
(2021) use Wikipedia hyperlinks to learn multilin-
gual LMs. Citation links are often used to improve
summarization and recommendation of academic
papers (Qazvinian and Radev, 2008; ","Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasu",2203.15827,"Language model (LM) pretraining can learn various knowledge from text
corpora, helping downstream tasks. However, existing methods such as BERT model
a single document, and do not capture dependencies or knowledge that span
across documents. In this work, we propose LinkBERT, an LM pretraining method
that leverages links between documents, e.g., hyperlinks. Given a text corpus,
we view it as a graph of documents and create LM inputs by placing linked
documents in the same context. We then pretrain the LM with two joint
self-supervised objectives: masked language modeling and our new proposal,
document relation prediction. We show that LinkBERT outperforms BERT on various
downstream tasks across two domains: the general domain (pretrained on
Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with
citation links). LinkBERT is especially effective for multi-hop reasoning and
few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our
biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on
BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT,
as well as code and data at https://github.com/michiyasunaga/LinkBERT."
"ers (Qazvinian and Radev, 2008; Yasunaga et al.,
2019; Bhagavatula et al., 2018; Khadka et al., 2020;
Cohan et al., 2020). Here we leverage citation net-
works to improve pretraining general-purpose LMs.
Graph-augmented LMs. Several works aug-
ment LMs with graphs, typically, knowledge graphs
(KGs) where the nodes capture entities and edges
their relations. ","Zhang et al. (2019); He et al. (2020);
Wang et al. (2021b) combine LM training with
KG embeddings. Sun et al. (2020); Yasunaga et al.
(2021); Zhang et al. (2022) combine LMs and graph
neural networks (GNNs) to jointly train on text and
KGs. Different from KGs, we use document graphs
to learn knowledge that spans across documents.
3 Preliminaries
A language m",1905.07129,"Neural language representation models such as BERT pre-trained on large-scale
corpora can well capture rich semantic patterns from plain text, and be
fine-tuned to consistently improve the performance of various NLP tasks.
However, the existing pre-trained language models rarely consider incorporating
knowledge graphs (KGs), which can provide rich structured knowledge facts for
better language understanding. We argue that informative entities in KGs can
enhance language representation with external knowledge. In this paper, we
utilize both large-scale textual corpora and KGs to train an enhanced language
representation model (ERNIE), which can take full advantage of lexical,
syntactic, and knowledge information simultaneously. The experimental results
have demonstrated that ERNIE achieves significant improvements on various
knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art
model BERT on other common NLP tasks. The source code of this paper can be
obtained from https://github.com/thunlp/ERNIE."
"§6). Hy-
perlinks have a number of advantages. They provide
background knowledge about concepts that the doc-
ument writers deemed useful—the links are likely
to have high precision of relevance, and can also
bring in relevant documents that may not be obvious
via lexical similarity alone (e.g., in Figure 1, while
the hyperlinked article mentions “Japanese” and
“Yoshino” cherry trees, these words do not appear in
the anchor article). Hyperlinks are also ubiquitous
on the web and easily gathered at scale (","Aghajanyan
et al., 2021). To construct the document graph, we
simply make a directed edge (X(i);X(j))if there is
a hyperlink from document X(i)to document X(j).
For comparison, we also experiment with a docu-
ment graph built by lexical similarity between docu-
ments. For each document X(i), we use the common
TF-IDF cosine similarity metric (Chen et al., 2017;
Yasunaga et al., 2017) to obtain top- kdocuments
X(j)’s and make edges (X(i);X(j)). We usek=5.
4.2 Pretraining tasks
Creating input instances. Seve",2107.06955,"We introduce HTLM, a hyper-text language model trained on a large-scale web
crawl. Modeling hyper-text has a number of advantages: (1) it is easily
gathered at scale, (2) it provides rich document-level and end-task-adjacent
supervision (e.g. class and id attributes often encode document category
information), and (3) it allows for new structured prompting that follows the
established semantics of HTML (e.g. to do zero-shot summarization by infilling
title tags for a webpage that contains the input text). We show that
pretraining with a BART-style denoising loss directly on simplified HTML
provides highly effective transfer for a wide range of end tasks and
supervision levels. HTLM matches or exceeds the performance of comparably sized
text-only LMs for zero-shot prompting and fine-tuning for classification
benchmarks, while also setting new state-of-the-art performance levels for
zero-shot summarization. We also find that hyper-text prompts provide more
value to HTLM, in terms of data efficiency, than plain text prompts do for
existing LMs, and that HTLM is highly effective at auto-prompting itself, by
simply generating the most likely hyper-text formatting for any available
training data. We will release all code and models to support future HTLM
research."
" and BookCorpus to train
LinkBERT. In summary, our pretraining data is
the same as BERT, except that we have hyperlinks
between Wikipedia articles.
Implementation. We pretrain LinkBERT of
three sizes, -tiny, -base and -large, following the
conﬁgurations of BERT tiny (4.4M parameters),
BERT base(110M params), and BERT large (340M
params) (Devlin et al., 2019;"," Turc et al., 2019). We
use -tiny mainly for ablation studies.
For -tiny, we pretrain from scratch with ran-
dom weight initialization. We use the AdamW
(Loshchilov and Hutter, 2019) optimizer with
(1;2) = (0:9;0:98), warm up the learning rate
for the ﬁrst 5,000 steps and then linearly decay it.
3https://github.com/attardi/wikiextractorWe train for 10,000 ",1908.08962,"Recent developments in natural language representations have been accompanied
by large and expensive models that leverage vast amounts of general-domain text
through self-supervised pre-training. Due to the cost of applying such models
to down-stream tasks, several model compression techniques on pre-trained
language representations have been proposed (Sun et al., 2019; Sanh, 2019).
However, surprisingly, the simple baseline of just pre-training and fine-tuning
compact models has been overlooked. In this paper, we first show that
pre-training remains important in the context of smaller architectures, and
fine-tuning pre-trained compact models can be competitive to more elaborate
methods proposed in concurrent work. Starting with pre-trained compact models,
we then explore transferring task knowledge from large fine-tuned models
through standard knowledge distillation. The resulting simple, yet effective
and general algorithm, Pre-trained Distillation, brings further improvements.
Through extensive experiments, we more generally explore the interaction
between pre-training and distillation under two variables that have been
under-studied: model size and properties of unlabeled task data. One surprising
observation is that they have a compound effect even when sequentially applied
on the same data. To accelerate future research, we will make our 24
pre-trained miniature BERT models publicly available."
"answering (QA). Given a
document (or set of documents) and a question as
input, the task is to identify an answer span from
the document. We evaluate on six popular datasets
from the MRQA shared task (Fisch et al., 2019):
HotpotQA (Yang et al., 2018), TriviaQA (Joshi
et al., 2017), NaturalQ (Kwiatkowski et al., 2019),
SearchQA (","Dunn et al., 2017), NewsQA (Trischler
et al., 2017), and SQuAD (Rajpurkar et al., 2016).
As the MRQA shared task does not have a public
test set, we split the dev set in half to make new
dev and test sets. We follow the ﬁne-tuning method
BERT (Devlin et al., 2019) uses for extractive QA.
More details are provided in Appendix B.
",1704.05179,"We publicly release a new large-scale dataset, called SearchQA, for machine
comprehension, or question-answering. Unlike recently released datasets, such
as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to
reflect a full pipeline of general question-answering. That is, we start not
from an existing article and generate a question-answer pair, but start from an
existing question-answer pair, crawled from J! Archive, and augment it with
text snippets retrieved by Google. Following this approach, we built SearchQA,
which consists of more than 140k question-answer pairs with each pair having
49.6 snippets on average. Each question-answer-context tuple of the SearchQA
comes with additional meta-data such as the snippet's URL, which we believe
will be valuable resources for future research. We conduct human evaluation as
well as test two baseline methods, one simple word selection and the other deep
learning based, on the SearchQA. We show that there is a meaningful gap between
the human and machine performances. This suggests that the proposed dataset
could well serve as a benchmark for question-answering."
